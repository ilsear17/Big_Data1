{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22bc4d93",
   "metadata": {},
   "source": [
    "# Ejercicio Práctico RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1a214d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c07408",
   "metadata": {},
   "source": [
    "**spark = SparkSession.builder.getOrCreate()**\n",
    "\n",
    "**SparkSession.builder**\n",
    "* Este es un método de construcción que se utiliza para configurar una nueva sesión de Spark. **SparkSession** es una clase que actúa como un punto de entrada para leer datos en Spark y para realizar varias operaciones en esos datos.\n",
    "\n",
    "**getOrCreate()**\n",
    "* Este método intenta obtener una **SparkSession** existente si hay una; de lo contrario, crea una nueva. Esencialmente, este método asegura que no termines con múltiples sesiones de Spark en tu aplicación, lo que podría causar problemas.\n",
    "\n",
    "**sc = spark.sparkContext**\n",
    "\n",
    "**spark.sparkContext**\n",
    "* Una vez que tienes una **SparkSession**, puedes utilizarla para obtener el **SparkContext** actual utilizando la propiedad sparkContext.\n",
    "**SparkContext**  es un cliente que se utiliza para interactuar con el clúster de Spark. Esencialmente, es el punto de entrada para cualquier funcionalidad de Spark.\n",
    "* Aunque con la introducción de SparkSession en Spark 2.0, **SparkContext** se utiliza menos directamente para las operaciones diarias de Spark, todavía es necesario para ciertas operaciones y para configurar algunas configuraciones a nivel más bajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd8b0d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5acc4fc",
   "metadata": {},
   "source": [
    "**RDD (Resilient Distributed Dataset) desde una lista de Python usando el método parallelize.**\n",
    "\n",
    "A continuación desgloso cada parte del código:\n",
    "\n",
    "**data= [1, 2, 3, 4, 5]**\n",
    "\n",
    "* Esta línea está creando una lista de Python que contiene cinco elementos (los números del 1 al 5).\n",
    "\n",
    "**myRDD= sc.parallelize(data)**\n",
    "\n",
    "* Esta línea está realizando varias operaciones:\n",
    "\n",
    "**sc.parallelize**\n",
    "\n",
    "* Este método del **SparkContext (sc)** está siendo usado para crear un RDD.\n",
    "* El método **parallelize** toma una colección existente en tu programa driver (en este caso, la lista **data**) y la distribuye a través de los nodos en tu clúster de Spark para crear un RDD.\n",
    "\n",
    "**(data)**\n",
    "\n",
    "* Estás pasando tu lista data como el argumento para el método **parallelize**. Esto le dice a Spark que quieres crear un RDD que contenga todos los elementos de **data**.\n",
    "\n",
    "**myRDD**\n",
    "\n",
    "Estás asignando el **RDD** resultante a una variable llamada **myRDD**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1479e36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data= [1, 2, 3, 4, 5]\n",
    "myRDD= sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be064bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 5]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filteredRDD = myRDD.filter(lambda x: x > 2)\n",
    "filteredRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb88014",
   "metadata": {},
   "source": [
    "El está aplicando una transformación map a myRDD, creando un nuevo RDD llamado newRDD. Aquí está el desglose de cada componente del código:\n",
    "\n",
    "**myRDD.map(lambda x: x*2)**\n",
    "\n",
    "* **myRDD:** Este es el RDD original que creaste a partir de tu lista de datos.\n",
    "* **map(...):** Este es un método de RDD que aplica una función a cada elemento en el RDD.\n",
    "* **lambda x: x*2:** Esta es una función lambda que toma un elemento x y devuelve ese elemento multiplicado por 2.\n",
    "\n",
    "**newRDD=**\n",
    "\n",
    "Estás asignando el nuevo **RDD** resultante a una variable llamada **newRDD**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bc25ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "newRDD= myRDD.map(lambda x: x*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99d4cb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8, 10]\n"
     ]
    }
   ],
   "source": [
    "result = newRDD.collect()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9b42c6",
   "metadata": {},
   "source": [
    "En el código  estás aplicando una transformación **map** al RDD **myRDD**. Dentro de esta transformación, estás utilizando una función lambda para generar un rango de números desde **1** hasta **x-1** (porque **range** excluye el valor final) para cada elemento **x** en el **myRDD**.\n",
    "\n",
    "\n",
    "En el código proporcionado, estás aplicando una transformación map al RDD myRDD. Dentro de esta transformación, estás utilizando una función lambda para generar un rango de números desde 1 hasta x-1 (porque range excluye el valor final) para cada elemento x en el myRDD. Aquí está el desglose del código:\n",
    "\n",
    "\n",
    "**myRDD.map(lambda x: range(1, x))**\n",
    "\n",
    "* **myRDD:** Este es tu RDD original que contiene los números del 1 al 5.\n",
    "* **map(...):** Este es un método de RDD que aplica una función a cada elemento del RDD.\n",
    "* **lambda x**: range(1, x): Esta es una función lambda que toma un elemento **x** y devuelve un objeto de rango que representa números desde **1** hasta **x-1**.\n",
    "\n",
    "**mapRDD=**\n",
    "\n",
    "* Estás asignando el nuevo RDD resultante a una variable llamada **mapRDD**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4bf5026",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapRDD= myRDD.map(lambda x: range(1,x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1e36a6",
   "metadata": {},
   "source": [
    "Este fragmento de código es un ejemplo de cómo usar Apache Spark, una plataforma de computación de clúster de código abierto diseñada para procesar y analizar grandes conjuntos de datos. Vamos a desglosar el código línea por línea:\n",
    "\n",
    "**result = mapRDD.collect()**\n",
    "\n",
    "* **mapRDD.collect():** Este método es usado para recuperar todos los elementos de un RDD (Resilient Distributed Dataset) de Spark a la memoria del controlador (o driver) como una lista de Python. El mapRDD es un RDD que se ha creado previamente mediante alguna operación de transformación (como map, filter, etc.) sobre un RDD original.\n",
    "\n",
    "**result_as_lists = [list(r) for r in result]**\n",
    "\n",
    "* **[list(r) for r in result]:** Este fragmento de código es una comprensión de lista (list comprehension) que itera sobre todos los elementos en la lista result y convierte cada elemento r en una lista de Python. Esto puede ser útil si, por ejemplo, cada elemento en result es un iterable (como una tupla) y deseas convertir cada iterable en una lista.\n",
    "\n",
    "**print(result_as_lists)**\n",
    "\n",
    "* **print(result_as_lists):** Finalmente, este comando imprime la representación de cadena de la lista result_as_lists a la consola, permitiendo que veas el contenido de result_as_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "668c54df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [1], [1, 2], [1, 2, 3], [1, 2, 3, 4]]\n"
     ]
    }
   ],
   "source": [
    "result = mapRDD.collect()\n",
    "result_as_lists = [list(r) for r in result]\n",
    "print(result_as_lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f29d69e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8c977db",
   "metadata": {},
   "source": [
    "El fragmento de código que has proporcionado es un ejemplo de cómo usar PySpark, la interfaz de Python para Apache Spark, para crear y manipular Resilient Distributed Datasets (RDDs). Desglosaremos cada línea del código para entender qué está haciendo:\n",
    "\n",
    "**data = [1, 2, 3, 4, 5, 6]**\n",
    "\n",
    "* En esta línea se está creando una lista de Python llamada data que contiene seis números enteros.\n",
    "\n",
    "\n",
    "**myRDD = sc.parallelize(data)**\n",
    "\n",
    "* Aquí, **sc.parallelize(data)** está llamando al método parallelize en el SparkContext (sc) para crear un RDD a partir de la lista data. Un RDD es una colección de elementos tolerante a fallos que puede ser operada en paralelo. En este caso, estamos creando un RDD que contiene los mismos elementos que la lista data.\n",
    "\n",
    "\n",
    "**newRDD = myRDD.filter(lambda x: x % 2 == 0)**\n",
    "\n",
    "* En esta línea, estamos usando el método filter para crear un nuevo RDD que contiene solo los elementos del myRDD que satisfacen la condición especificada por la función lambda, que es retener solo los números que son divisibles por 2 (es decir, solo los números pares). La función lambda toma un argumento x y retorna True si x es divisible por 2 y False de lo contrario. Entonces, newRDD será un RDD que contiene solo los números 2, 4 y 6.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a047081a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data= [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "myRDD= sc.parallelize(data)\n",
    "newRDD= myRDD.filter(lambda x: x%2 == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3597b25",
   "metadata": {},
   "source": [
    "El método distinct en PySpark se utiliza para eliminar elementos duplicados de un RDD. Cuando aplicas este método a un RDD, obtienes un nuevo RDD que contiene solo elementos únicos del RDD original.\n",
    "\n",
    "En tu fragmento de código:\n",
    "\n",
    "**newRDD = myRDD.distinct()**\n",
    "\n",
    "* Estás creando un newRDD que contiene todos los elementos únicos de myRDD. Si myRDD contiene elementos duplicados, estos serán eliminados en newRDD.\n",
    "\n",
    "* Por ejemplo, si myRDD se creó a partir de una lista que contiene elementos duplicados como la siguiente:\n",
    "\n",
    "           data = [1, 2, 2, 3, 3, 3, 4, 5, 6, 6]\n",
    "\n",
    "           myRDD = sc.parallelize(data)\n",
    "\n",
    "* Entonces, después de aplicar el método distinct, newRDD contendrá cada número del 1 al 6 exactamente una vez, eliminando las duplicaciones presentes en myRDD.\n",
    "\n",
    "* Es importante tener en cuenta que el método distinct puede causar una gran cantidad de operaciones de shuffle de datos, especialmente si el RDD original tiene un gran número de elementos duplicados, lo que puede resultar en un proceso bastante costoso en términos de tiempo y recursos computacionales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aeb368f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "newRDD= myRDD.distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ef8ef7",
   "metadata": {},
   "source": [
    "El fragmento de código que has proporcionado es otro ejemplo de cómo trabajar con PySpark para manipular RDDs. Aquí está el desglose línea por línea:\n",
    "\n",
    "\n",
    "**from operator import add**\n",
    "\n",
    "Aquí estás importando la función add del módulo operator. Esta función toma dos números como argumentos y devuelve su suma.\n",
    "\n",
    "\n",
    "**myRDD = sc.parallelize([('a', 1), ('a', 2), ('a', 3), ('b', 1)])**\n",
    "\n",
    "En esta línea, estás creando un RDD llamado myRDD a partir de una lista de tuplas, donde cada tupla consta de una letra (una cadena) y un número.\n",
    "\n",
    "\n",
    "**newRDD= myRDD.reduceByKey(add)**\n",
    "\n",
    "Aquí estás usando el método reduceByKey para combinar los valores de **myRDD** que tienen la misma clave (en este caso, la primera componente de cada tupla) aplicando la función add.\n",
    "\n",
    "El resultado, **newRDD**, será un RDD que contiene solo dos elementos: una tupla con la clave **'a'** y la suma de todos los valores asociados con la clave **'a'** en **myRDD** (que es 6) y una tupla con la clave **'b'** y la suma de todos los valores asociados con la clave **'b'** en **myRDD** (que es 1).\n",
    "\n",
    "Por lo tanto, **newRDD** será algo como esto:\n",
    "\n",
    "**[('a', 6), ('b', 1)]**\n",
    "\n",
    "o puede que el orden sea inverso dependiendo de cómo Spark decida ordenar las claves:\n",
    "\n",
    "\n",
    "**[('b', 1), ('a', 6)]**\n",
    "\n",
    "Este proceso es una parte fundamental de la programación MapReduce, que es el paradigma de programación subyacente de Spark. En esta operación específica, estás realizando la \"reducción\" de los datos, donde los valores para cada clave son combinados (reducidos) en un solo valor usando la función de adición (add)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "787c7986",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "\n",
    "myRDD = sc.parallelize([('a', 1), ('a', 2), ('a', 3), ('b', 1)])\n",
    "newRDD= myRDD.reduceByKey(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5133d6f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('a', 2), ('a', 3), ('b', 1)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newRDD= myRDD.sortByKey()\n",
    "newRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bfe2d705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data= [1, 2, 3, 4, 5]\n",
    "\n",
    "myRDD= sc.parallelize(data)\n",
    "myRDD.reduce( lambda x, y: x * y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6139be0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('Python', 3), ('Scala', 1), ('R', 2), ('Java', 1)])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data= ['Python', 'Scala', 'Python', 'R', 'Python', 'Java', 'R' ]\n",
    "\n",
    "myRDD= sc.parallelize(data)\n",
    "\n",
    "myRDD.countByValue().items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "633f7716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('a', 2), ('b', 1), ('c', 1)])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data= [('a', 1), ('b', 1), ('c', 1), ('a', 1)]\n",
    "myRDD = sc.parallelize(data)\n",
    "\n",
    "myRDD.countByKey().items()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
