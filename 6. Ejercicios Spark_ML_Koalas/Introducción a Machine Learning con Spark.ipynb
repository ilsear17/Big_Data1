{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   MLlib: La librería de aprendizaje automático escalable de Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este notebook aprenderemos a entrenar modelos de Machine Learning con Spark, utilizando la librería de MLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jupyter_contrib_nbextensions\n",
      "  Downloading jupyter_contrib_nbextensions-0.7.0.tar.gz (23.5 MB)\n",
      "Requirement already satisfied: ipython_genutils in c:\\users\\ilse-\\anaconda3\\envs\\bigdata2\\lib\\site-packages (from jupyter_contrib_nbextensions) (0.2.0)\n",
      "Collecting jupyter_contrib_core>=0.3.3\n",
      "  Downloading jupyter_contrib_core-0.4.2.tar.gz (17 kB)\n",
      "Requirement already satisfied: jupyter_core in c:\\users\\ilse-\\anaconda3\\envs\\bigdata2\\lib\\site-packages (from jupyter_contrib_nbextensions) (4.9.2)\n",
      "Collecting jupyter_highlight_selected_word>=0.1.1\n",
      "  Downloading jupyter_highlight_selected_word-0.2.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting jupyter_nbextensions_configurator>=0.4.0\n",
      "  Downloading jupyter_nbextensions_configurator-0.6.3-py2.py3-none-any.whl (466 kB)\n",
      "Collecting nbconvert>=6.0\n",
      "  Downloading nbconvert-6.0.7-py3-none-any.whl (552 kB)\n",
      "Collecting notebook>=6.0\n",
      "  Downloading notebook-6.4.10-py3-none-any.whl (9.9 MB)\n",
      "Requirement already satisfied: tornado in c:\\users\\ilse-\\anaconda3\\envs\\bigdata2\\lib\\site-packages (from jupyter_contrib_nbextensions) (6.1)\n",
      "Requirement already satisfied: traitlets>=4.1 in c:\\users\\ilse-\\anaconda3\\envs\\bigdata2\\lib\\site-packages (from jupyter_contrib_nbextensions) (4.3.3)\n",
      "Collecting lxml\n",
      "  Downloading lxml-4.9.3-cp36-cp36m-win_amd64.whl (3.8 MB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ilse-\\anaconda3\\envs\\bigdata2\\lib\\site-packages (from jupyter_contrib_core>=0.3.3->jupyter_contrib_nbextensions) (58.0.4)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\ilse-\\anaconda3\\envs\\bigdata2\\lib\\site-packages (from jupyter_nbextensions_configurator>=0.4.0->jupyter_contrib_nbextensions) (6.0.1)\n",
      "Requirement already satisfied: nbformat>=4.4 in c:\\users\\ilse-\\anaconda3\\envs\\bigdata2\\lib\\site-packages (from nbconvert>=6.0->jupyter_contrib_nbextensions) (5.1.3)\n",
      "Collecting testpath\n",
      "  Downloading testpath-0.6.0-py3-none-any.whl (83 kB)\n",
      "Collecting defusedxml\n",
      "  Downloading defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Collecting mistune<2,>=0.8.1\n",
      "  Downloading mistune-0.8.4-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in c:\\users\\ilse-\\anaconda3\\envs\\bigdata2\\lib\\site-packages (from nbconvert>=6.0->jupyter_contrib_nbextensions) (0.4)\n",
      "Collecting bleach\n",
      "  Downloading bleach-4.1.0-py2.py3-none-any.whl (157 kB)\n",
      "Collecting pandocfilters>=1.4.1\n",
      "  Downloading pandocfilters-1.5.0-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting jupyterlab-pygments\n",
      "  Downloading jupyterlab_pygments-0.1.2-py2.py3-none-any.whl (4.6 kB)\n",
      "Requirement already satisfied: pygments>=2.4.1 in c:\\users\\ilse-\\anaconda3\\envs\\bigdata2\\lib\\site-packages (from nbconvert>=6.0->jupyter_contrib_nbextensions) (2.14.0)\n",
      "Requirement already satisfied: jinja2>=2.4 in c:\\users\\ilse-\\anaconda3\\envs\\bigdata2\\lib\\site-packages (from nbconvert>=6.0->jupyter_contrib_nbextensions) (3.0.3)\n",
      "Collecting nbclient<0.6.0,>=0.5.0\n",
      "  Downloading nbclient-0.5.9-py3-none-any.whl (69 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ilse-\\anaconda3\\envs\\bigdata2\\lib\\site-packages (from jinja2>=2.4->nbconvert>=6.0->jupyter_contrib_nbextensions) (2.0.1)\n",
      "Requirement already satisfied: jupyter-client>=6.1.5 in c:\\users\\ilse-\\anaconda3\\envs\\bigdata2\\lib\\site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert>=6.0->jupyter_contrib_nbextensions) (7.1.2)\n",
      "Collecting async-generator\n",
      "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\ilse-\\anaconda3\\envs\\bigdata2\\lib\\site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert>=6.0->jupyter_contrib_nbextensions) (1.5.8)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\ilse-\\anaconda3\\envs\\bigdata2\\lib\\site-packages (from jupyter-client>=6.1.5->nbclient<0.6.0,>=0.5.0->nbconvert>=6.0->jupyter_contrib_nbextensions) (2.8.2)\n",
      "Requirement already satisfied: pyzmq>=13 in c:\\users\\ilse-\\anaconda3\\envs\\bigdata2\\lib\\site-packages (from jupyter-client>=6.1.5->nbclient<0.6.0,>=0.5.0->nbconvert>=6.0->jupyter_contrib_nbextensions) (22.3.0)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\users\\ilse-\\anaconda3\\envs\\bigdata2\\lib\\site-packages (from jupyter_core->jupyter_contrib_nbextensions) (305)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in c:\\users\\ilse-\\anaconda3\\envs\\bigdata2\\lib\\site-packages (from nbformat>=4.4->nbconvert>=6.0->jupyter_contrib_nbextensions) (3.2.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\ilse-\\anaconda3\\envs\\bigdata2\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert>=6.0->jupyter_contrib_nbextensions) (22.2.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\users\\ilse-\\anaconda3\\envs\\bigdata2\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert>=6.0->jupyter_contrib_nbextensions) (0.18.0)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\ilse-\\anaconda3\\envs\\bigdata2\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert>=6.0->jupyter_contrib_nbextensions) (4.8.3)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\ilse-\\anaconda3\\envs\\bigdata2\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert>=6.0->jupyter_contrib_nbextensions) (1.16.0)\n",
      "Collecting Send2Trash>=1.8.0\n",
      "  Downloading Send2Trash-1.8.2-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\ilse-\\anaconda3\\envs\\bigdata2\\lib\\site-packages (from notebook>=6.0->jupyter_contrib_nbextensions) (5.5.5)\n",
      "Collecting argon2-cffi\n",
      "  Downloading argon2_cffi-21.3.0-py3-none-any.whl (14 kB)\n",
      "Collecting terminado>=0.8.3\n",
      "  Downloading terminado-0.12.1-py3-none-any.whl (15 kB)\n",
      "Collecting prometheus-client\n",
      "  Downloading prometheus_client-0.17.1-py3-none-any.whl (60 kB)\n",
      "Collecting pywinpty>=1.1.0\n",
      "  Downloading pywinpty-2.0.3.tar.gz (23 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'error'\n",
      "  Downloading pywinpty-2.0.2.tar.gz (22 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'error'\n",
      "  Downloading pywinpty-2.0.1.tar.gz (22 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'error'\n",
      "  Downloading pywinpty-1.1.6-cp36-none-win_amd64.whl (1.4 MB)\n",
      "Requirement already satisfied: decorator in c:\\users\\ilse-\\anaconda3\\envs\\bigdata2\\lib\\site-packages (from traitlets>=4.1->jupyter_contrib_nbextensions) (5.1.1)\n",
      "Collecting dataclasses\n",
      "  Downloading dataclasses-0.8-py3-none-any.whl (19 kB)\n",
      "Collecting argon2-cffi-bindings\n",
      "  Downloading argon2_cffi_bindings-21.2.0-cp36-abi3-win_amd64.whl (30 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\ilse-\\anaconda3\\envs\\bigdata2\\lib\\site-packages (from argon2-cffi->notebook>=6.0->jupyter_contrib_nbextensions) (4.1.1)\n",
      "Collecting cffi>=1.0.1\n",
      "  Downloading cffi-1.15.1-cp36-cp36m-win_amd64.whl (187 kB)\n",
      "Collecting pycparser\n",
      "  Downloading pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\ilse-\\anaconda3\\envs\\bigdata2\\lib\\site-packages (from bleach->nbconvert>=6.0->jupyter_contrib_nbextensions) (21.3)\n",
      "Collecting webencodings\n",
      "  Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\ilse-\\anaconda3\\envs\\bigdata2\\lib\\site-packages (from importlib-metadata->jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert>=6.0->jupyter_contrib_nbextensions) (3.6.0)\n",
      "Requirement already satisfied: ipython>=5.0.0 in c:\\users\\ilse-\\anaconda3\\envs\\bigdata2\\lib\\site-packages (from ipykernel->notebook>=6.0->jupyter_contrib_nbextensions) (7.16.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\ilse-\\anaconda3\\envs\\bigdata2\\lib\\site-packages (from ipython>=5.0.0->ipykernel->notebook>=6.0->jupyter_contrib_nbextensions) (0.4.5)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\ilse-\\anaconda3\\envs\\bigdata2\\lib\\site-packages (from ipython>=5.0.0->ipykernel->notebook>=6.0->jupyter_contrib_nbextensions) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\ilse-\\anaconda3\\envs\\bigdata2\\lib\\site-packages (from ipython>=5.0.0->ipykernel->notebook>=6.0->jupyter_contrib_nbextensions) (3.0.36)\n",
      "Requirement already satisfied: backcall in c:\\users\\ilse-\\anaconda3\\envs\\bigdata2\\lib\\site-packages (from ipython>=5.0.0->ipykernel->notebook>=6.0->jupyter_contrib_nbextensions) (0.2.0)\n",
      "Requirement already satisfied: jedi>=0.10 in c:\\users\\ilse-\\anaconda3\\envs\\bigdata2\\lib\\site-packages (from ipython>=5.0.0->ipykernel->notebook>=6.0->jupyter_contrib_nbextensions) (0.17.2)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in c:\\users\\ilse-\\anaconda3\\envs\\bigdata2\\lib\\site-packages (from jedi>=0.10->ipython>=5.0.0->ipykernel->notebook>=6.0->jupyter_contrib_nbextensions) (0.7.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\ilse-\\anaconda3\\envs\\bigdata2\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.0.0->ipykernel->notebook>=6.0->jupyter_contrib_nbextensions) (0.2.10)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\ilse-\\anaconda3\\envs\\bigdata2\\lib\\site-packages (from packaging->bleach->nbconvert>=6.0->jupyter_contrib_nbextensions) (3.1.1)\n",
      "Building wheels for collected packages: jupyter-contrib-nbextensions, jupyter-contrib-core\n",
      "  Building wheel for jupyter-contrib-nbextensions (setup.py): started\n",
      "  Building wheel for jupyter-contrib-nbextensions (setup.py): finished with status 'done'\n",
      "  Created wheel for jupyter-contrib-nbextensions: filename=jupyter_contrib_nbextensions-0.7.0-py2.py3-none-any.whl size=23428787 sha256=41bc2053b79929e22bc97b7d0a5aabbafd7172c2dae0a1a2d87e2369ee1c2a6a\n",
      "  Stored in directory: c:\\users\\ilse-\\appdata\\local\\pip\\cache\\wheels\\bc\\30\\f6\\b11297a115e4266004e1ca9fe4c45bb1530218cec8dc6d6c34\n",
      "  Building wheel for jupyter-contrib-core (setup.py): started\n",
      "  Building wheel for jupyter-contrib-core (setup.py): finished with status 'done'\n",
      "  Created wheel for jupyter-contrib-core: filename=jupyter_contrib_core-0.4.2-py2.py3-none-any.whl size=17490 sha256=4c2f70d8da45ec0290b4961f172a26871f801919657579a90f9a61ff160c2e23\n",
      "  Stored in directory: c:\\users\\ilse-\\appdata\\local\\pip\\cache\\wheels\\32\\de\\59\\bafe20bb2fcdd06d67287ec2e2c69c7522fda9a640fdf01c3c\n",
      "Successfully built jupyter-contrib-nbextensions jupyter-contrib-core\n",
      "Installing collected packages: pycparser, webencodings, cffi, async-generator, testpath, pywinpty, pandocfilters, nbclient, mistune, jupyterlab-pygments, defusedxml, dataclasses, bleach, argon2-cffi-bindings, terminado, Send2Trash, prometheus-client, nbconvert, argon2-cffi, notebook, jupyter-contrib-core, lxml, jupyter-nbextensions-configurator, jupyter-highlight-selected-word, jupyter-contrib-nbextensions\n",
      "Successfully installed Send2Trash-1.8.2 argon2-cffi-21.3.0 argon2-cffi-bindings-21.2.0 async-generator-1.10 bleach-4.1.0 cffi-1.15.1 dataclasses-0.8 defusedxml-0.7.1 jupyter-contrib-core-0.4.2 jupyter-contrib-nbextensions-0.7.0 jupyter-highlight-selected-word-0.2.0 jupyter-nbextensions-configurator-0.6.3 jupyterlab-pygments-0.1.2 lxml-4.9.3 mistune-0.8.4 nbclient-0.5.9 nbconvert-6.0.7 notebook-6.4.10 pandocfilters-1.5.0 prometheus-client-0.17.1 pycparser-2.21 pywinpty-1.1.6 terminado-0.12.1 testpath-0.6.0 webencodings-0.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'c:\\Users\\ilse-\\anaconda3\\envs\\bigdata2\\python.exe' 'c:\\Users\\ilse-\\anaconda3\\envs\\bigdata2\\lib\\site-packages\\pip\\_vendor\\pep517\\in_process\\_in_process.py' prepare_metadata_for_build_wheel 'C:\\Users\\ilse-\\AppData\\Local\\Temp\\tmp1zy4rbiw'\n",
      "         cwd: C:\\Users\\ilse-\\AppData\\Local\\Temp\\pip-install-vniqxc1p\\pywinpty_b02ed3c9686b4066a70ab5f3e0279073\n",
      "    Complete output (6 lines):\n",
      "    Checking for Rust toolchain....\n",
      "    \n",
      "    Cargo, the Rust package manager, is not installed or is not on PATH.\n",
      "    This package requires Rust and Cargo to compile extensions. Install it through\n",
      "    the system's package manager or via https://rustup.rs/\n",
      "    \n",
      "    ----------------------------------------\n",
      "WARNING: Discarding https://files.pythonhosted.org/packages/c3/61/bda90dba80bc6cb905acebd0bf0710777ab04feb29d0f438202da0e82d72/pywinpty-2.0.3.tar.gz#sha256=6b29a826e896105370c38d53904c3aaac6c36146a50448fc0ed5082cf9d092bc (from https://pypi.org/simple/pywinpty/) (requires-python:>=3.6). Command errored out with exit status 1: 'c:\\Users\\ilse-\\anaconda3\\envs\\bigdata2\\python.exe' 'c:\\Users\\ilse-\\anaconda3\\envs\\bigdata2\\lib\\site-packages\\pip\\_vendor\\pep517\\in_process\\_in_process.py' prepare_metadata_for_build_wheel 'C:\\Users\\ilse-\\AppData\\Local\\Temp\\tmp1zy4rbiw' Check the logs for full command output.\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'c:\\Users\\ilse-\\anaconda3\\envs\\bigdata2\\python.exe' 'c:\\Users\\ilse-\\anaconda3\\envs\\bigdata2\\lib\\site-packages\\pip\\_vendor\\pep517\\in_process\\_in_process.py' prepare_metadata_for_build_wheel 'C:\\Users\\ilse-\\AppData\\Local\\Temp\\tmpdad1u6so'\n",
      "         cwd: C:\\Users\\ilse-\\AppData\\Local\\Temp\\pip-install-vniqxc1p\\pywinpty_20d326dd772840748eee3ed7c166df4f\n",
      "    Complete output (6 lines):\n",
      "    Checking for Rust toolchain....\n",
      "    \n",
      "    Cargo, the Rust package manager, is not installed or is not on PATH.\n",
      "    This package requires Rust and Cargo to compile extensions. Install it through\n",
      "    the system's package manager or via https://rustup.rs/\n",
      "    \n",
      "    ----------------------------------------\n",
      "WARNING: Discarding https://files.pythonhosted.org/packages/43/88/f7d9b3b45a8b6109f60702cfa3b011edf7cfd8d2680f738bc4d1cb120897/pywinpty-2.0.2.tar.gz#sha256=20ec117183f79642eff555ce0dd1823f942618d65813fb6122d14b6e34b5d05a (from https://pypi.org/simple/pywinpty/) (requires-python:>=3.6). Command errored out with exit status 1: 'c:\\Users\\ilse-\\anaconda3\\envs\\bigdata2\\python.exe' 'c:\\Users\\ilse-\\anaconda3\\envs\\bigdata2\\lib\\site-packages\\pip\\_vendor\\pep517\\in_process\\_in_process.py' prepare_metadata_for_build_wheel 'C:\\Users\\ilse-\\AppData\\Local\\Temp\\tmpdad1u6so' Check the logs for full command output.\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'c:\\Users\\ilse-\\anaconda3\\envs\\bigdata2\\python.exe' 'c:\\Users\\ilse-\\anaconda3\\envs\\bigdata2\\lib\\site-packages\\pip\\_vendor\\pep517\\in_process\\_in_process.py' prepare_metadata_for_build_wheel 'C:\\Users\\ilse-\\AppData\\Local\\Temp\\tmpj2jgpc0j'\n",
      "         cwd: C:\\Users\\ilse-\\AppData\\Local\\Temp\\pip-install-vniqxc1p\\pywinpty_294ec46256d846af8eeaa4476121bd97\n",
      "    Complete output (6 lines):\n",
      "    Checking for Rust toolchain....\n",
      "    \n",
      "    Cargo, the Rust package manager, is not installed or is not on PATH.\n",
      "    This package requires Rust and Cargo to compile extensions. Install it through\n",
      "    the system's package manager or via https://rustup.rs/\n",
      "    \n",
      "    ----------------------------------------\n",
      "WARNING: Discarding https://files.pythonhosted.org/packages/b0/85/4a563e195fd5de30e4e18c51a4c398adbcbf656ca9166182ee44e2454719/pywinpty-2.0.1.tar.gz#sha256=14e7321c6d43743af0de175fca9f111c5cc8d0a9f7c608c9e1cc69ec0d6ac146 (from https://pypi.org/simple/pywinpty/) (requires-python:>=3.6). Command errored out with exit status 1: 'c:\\Users\\ilse-\\anaconda3\\envs\\bigdata2\\python.exe' 'c:\\Users\\ilse-\\anaconda3\\envs\\bigdata2\\lib\\site-packages\\pip\\_vendor\\pep517\\in_process\\_in_process.py' prepare_metadata_for_build_wheel 'C:\\Users\\ilse-\\AppData\\Local\\Temp\\tmpj2jgpc0j' Check the logs for full command output.\n"
     ]
    }
   ],
   "source": [
    "pip install jupyter_contrib_nbextensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fase 1. Importación y análisis de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pandas as pd\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('Titanic Data') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-RE9M0KP:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Titanic Data</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1eb290f93c8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.read\n",
    "          .format(\"csv\")\n",
    "          .option('header', 'true')\n",
    "          .load(\"C:/Users/ilse-/BIG_DATA1/Big_Data1/4. Ejercicios Spark_ML_Koalas/train.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex|Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male| 22|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female| 38|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female| 26|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female| 35|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male| 35|    0|    0|          373450|   8.05| null|       S|\n",
      "+-----------+--------+------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.25</td>\n",
       "      <td>None</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.925</td>\n",
       "      <td>None</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.05</td>\n",
       "      <td>None</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  PassengerId Survived Pclass  \\\n",
       "0           1        0      3   \n",
       "1           2        1      1   \n",
       "2           3        1      3   \n",
       "3           4        1      1   \n",
       "4           5        0      3   \n",
       "\n",
       "                                                Name     Sex Age SibSp Parch  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22     1     0   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38     1     0   \n",
       "2                             Heikkinen, Miss. Laina  female  26     0     0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35     1     0   \n",
       "4                           Allen, Mr. William Henry    male  35     0     0   \n",
       "\n",
       "             Ticket     Fare Cabin Embarked  \n",
       "0         A/5 21171     7.25  None        S  \n",
       "1          PC 17599  71.2833   C85        C  \n",
       "2  STON/O2. 3101282    7.925  None        S  \n",
       "3            113803     53.1  C123        S  \n",
       "4            373450     8.05  None        S  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex|Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male| 22|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female| 38|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female| 26|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female| 35|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male| 35|    0|    0|          373450|   8.05| null|       S|\n",
      "+-----------+--------+------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "891"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PassengerId', 'string'),\n",
       " ('Survived', 'string'),\n",
       " ('Pclass', 'string'),\n",
       " ('Name', 'string'),\n",
       " ('Sex', 'string'),\n",
       " ('Age', 'string'),\n",
       " ('SibSp', 'string'),\n",
       " ('Parch', 'string'),\n",
       " ('Ticket', 'string'),\n",
       " ('Fare', 'string'),\n",
       " ('Cabin', 'string'),\n",
       " ('Embarked', 'string')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "data_schema = [\n",
    "               StructField('PassengerId', IntegerType(), True),\n",
    "               StructField('Survived', StringType(), True),\n",
    "               StructField('Pclass', IntegerType(), True),\n",
    "               StructField('Name', StringType(), True),\n",
    "               StructField('Sex', StringType(), True),\n",
    "               StructField('Age', IntegerType(), True),\n",
    "               StructField('SibSp', IntegerType(), True),\n",
    "               StructField('Parch', IntegerType(), True),\n",
    "               StructField('Ticket', StringType(), True),\n",
    "               StructField('Fare', DoubleType(), True),\n",
    "               StructField('Cabin', StringType(), True),\n",
    "               StructField('Embarked', StringType(), True),\n",
    "            ]\n",
    "\n",
    "final_struc = StructType(fields=data_schema)\n",
    "\n",
    "df = spark.read.csv(\n",
    "    'C:/Users/ilse-/BIG_DATA1/Big_Data1/4. Ejercicios Spark_ML_Koalas/train.csv',\n",
    "    sep = ',',\n",
    "    header = True,\n",
    "    schema = final_struc\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: string (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>866</td>\n",
       "      <td>866</td>\n",
       "      <td>866</td>\n",
       "      <td>866</td>\n",
       "      <td>866</td>\n",
       "      <td>689</td>\n",
       "      <td>866</td>\n",
       "      <td>866</td>\n",
       "      <td>866</td>\n",
       "      <td>866</td>\n",
       "      <td>200</td>\n",
       "      <td>864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>446.97113163972284</td>\n",
       "      <td>0.3856812933025404</td>\n",
       "      <td>2.300230946882217</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>29.847605224963715</td>\n",
       "      <td>0.5277136258660509</td>\n",
       "      <td>0.37759815242494227</td>\n",
       "      <td>264373.1236306729</td>\n",
       "      <td>32.57242505773669</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>256.37755819268807</td>\n",
       "      <td>0.4870371056082385</td>\n",
       "      <td>0.8397219150282761</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>14.317668714749662</td>\n",
       "      <td>1.1131556315680113</td>\n",
       "      <td>0.8071396754421648</td>\n",
       "      <td>478381.3667343646</td>\n",
       "      <td>50.1330153452252</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Andersson, Mr. August Edvard (\"\"Wennerstrom\"\")\"</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>110152</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A10</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>891</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>van Melkebeke, Mr. Philemon</td>\n",
       "      <td>male</td>\n",
       "      <td>80</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>WE/P 5735</td>\n",
       "      <td>512.3292</td>\n",
       "      <td>T</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary         PassengerId            Survived              Pclass  \\\n",
       "0   count                 866                 866                 866   \n",
       "1    mean  446.97113163972284  0.3856812933025404   2.300230946882217   \n",
       "2  stddev  256.37755819268807  0.4870371056082385  0.8397219150282761   \n",
       "3     min                   1                   0                   1   \n",
       "4     max                 891                   1                   3   \n",
       "\n",
       "                                               Name     Sex  \\\n",
       "0                                               866     866   \n",
       "1                                              None    None   \n",
       "2                                              None    None   \n",
       "3  \"Andersson, Mr. August Edvard (\"\"Wennerstrom\"\")\"  female   \n",
       "4                       van Melkebeke, Mr. Philemon    male   \n",
       "\n",
       "                  Age               SibSp                Parch  \\\n",
       "0                 689                 866                  866   \n",
       "1  29.847605224963715  0.5277136258660509  0.37759815242494227   \n",
       "2  14.317668714749662  1.1131556315680113   0.8071396754421648   \n",
       "3                   1                   0                    0   \n",
       "4                  80                   8                    6   \n",
       "\n",
       "              Ticket               Fare Cabin Embarked  \n",
       "0                866                866   200      864  \n",
       "1  264373.1236306729  32.57242505773669  None     None  \n",
       "2  478381.3667343646   50.1330153452252  None     None  \n",
       "3             110152                0.0   A10        C  \n",
       "4          WE/P 5735           512.3292     T        S  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basics stats from our columns\n",
    "df.describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pandas_profiling import ProfileReport\n",
    "\n",
    "#df_pandas = df.toPandas()\n",
    "\n",
    "#pfr = ProfileReport(df_pandas)\n",
    "#pfr.to_notebook_iframe()\n",
    "#pfr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fase 2. Pre-procesamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+----+-------+--------+\n",
      "|Survived|Pclass|   Sex| Age|   Fare|Embarked|\n",
      "+--------+------+------+----+-------+--------+\n",
      "|     0.0|   3.0|  male|22.0|   7.25|       S|\n",
      "|     1.0|   1.0|female|38.0|71.2833|       C|\n",
      "|     1.0|   3.0|female|26.0|  7.925|       S|\n",
      "|     1.0|   1.0|female|35.0|   53.1|       S|\n",
      "|     0.0|   3.0|  male|35.0|   8.05|       S|\n",
      "|     0.0|   3.0|  male|null| 8.4583|       Q|\n",
      "|     0.0|   1.0|  male|54.0|51.8625|       S|\n",
      "|     0.0|   3.0|  male| 2.0| 21.075|       S|\n",
      "|     1.0|   3.0|female|27.0|11.1333|       S|\n",
      "|     1.0|   2.0|female|14.0|30.0708|       C|\n",
      "|     1.0|   3.0|female| 4.0|   16.7|       S|\n",
      "|     1.0|   1.0|female|58.0|  26.55|       S|\n",
      "|     0.0|   3.0|  male|20.0|   8.05|       S|\n",
      "|     0.0|   3.0|  male|39.0| 31.275|       S|\n",
      "|     0.0|   3.0|female|14.0| 7.8542|       S|\n",
      "|     1.0|   2.0|female|55.0|   16.0|       S|\n",
      "|     0.0|   3.0|  male| 2.0| 29.125|       Q|\n",
      "|     1.0|   2.0|  male|null|   13.0|       S|\n",
      "|     0.0|   3.0|female|31.0|   18.0|       S|\n",
      "|     1.0|   3.0|female|null|  7.225|       C|\n",
      "+--------+------+------+----+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "dataset = df.select(col('Survived').cast('float'),\n",
    "                         col('Pclass').cast('float'),\n",
    "                         col('Sex'),\n",
    "                         col('Age').cast('float'),\n",
    "                         col('Fare').cast('float'),\n",
    "                         col('Embarked')\n",
    "                        )\n",
    "dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+---+---+----+--------+\n",
      "|Survived|Pclass|Sex|Age|Fare|Embarked|\n",
      "+--------+------+---+---+----+--------+\n",
      "|      25|    25| 25|202|  25|      27|\n",
      "+--------+------+---+---+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnull, when, count, col\n",
    "\n",
    "dataset.select([count(when(isnull(c), c)).alias(c) for c in dataset.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing values\n",
    "dataset = dataset.replace('null', None)\\\n",
    "        .dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+----+-------+--------+------+-------+\n",
      "|Survived|Pclass|   Sex| Age|   Fare|Embarked|Gender|Boarded|\n",
      "+--------+------+------+----+-------+--------+------+-------+\n",
      "|     0.0|   3.0|  male|22.0|   7.25|       S|   0.0|    0.0|\n",
      "|     1.0|   1.0|female|38.0|71.2833|       C|   1.0|    1.0|\n",
      "|     1.0|   3.0|female|26.0|  7.925|       S|   1.0|    0.0|\n",
      "|     1.0|   1.0|female|35.0|   53.1|       S|   1.0|    0.0|\n",
      "|     0.0|   3.0|  male|35.0|   8.05|       S|   0.0|    0.0|\n",
      "|     0.0|   1.0|  male|54.0|51.8625|       S|   0.0|    0.0|\n",
      "|     0.0|   3.0|  male| 2.0| 21.075|       S|   0.0|    0.0|\n",
      "|     1.0|   3.0|female|27.0|11.1333|       S|   1.0|    0.0|\n",
      "|     1.0|   2.0|female|14.0|30.0708|       C|   1.0|    1.0|\n",
      "|     1.0|   3.0|female| 4.0|   16.7|       S|   1.0|    0.0|\n",
      "|     1.0|   1.0|female|58.0|  26.55|       S|   1.0|    0.0|\n",
      "|     0.0|   3.0|  male|20.0|   8.05|       S|   0.0|    0.0|\n",
      "|     0.0|   3.0|  male|39.0| 31.275|       S|   0.0|    0.0|\n",
      "|     0.0|   3.0|female|14.0| 7.8542|       S|   1.0|    0.0|\n",
      "|     1.0|   2.0|female|55.0|   16.0|       S|   1.0|    0.0|\n",
      "|     0.0|   3.0|  male| 2.0| 29.125|       Q|   0.0|    2.0|\n",
      "|     0.0|   3.0|female|31.0|   18.0|       S|   1.0|    0.0|\n",
      "|     0.0|   2.0|  male|35.0|   26.0|       S|   0.0|    0.0|\n",
      "|     1.0|   2.0|  male|34.0|   13.0|       S|   0.0|    0.0|\n",
      "|     1.0|   3.0|female|15.0| 8.0292|       Q|   1.0|    2.0|\n",
      "+--------+------+------+----+-------+--------+------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "dataset = StringIndexer(\n",
    "    inputCol='Sex', \n",
    "    outputCol='Gender', \n",
    "    handleInvalid='keep').fit(dataset).transform(dataset)\n",
    "\n",
    "dataset = StringIndexer(\n",
    "    inputCol='Embarked', \n",
    "    outputCol='Boarded', \n",
    "    handleInvalid='keep').fit(dataset).transform(dataset)\n",
    "\n",
    "dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Survived', 'float'),\n",
       " ('Pclass', 'float'),\n",
       " ('Sex', 'string'),\n",
       " ('Age', 'float'),\n",
       " ('Fare', 'float'),\n",
       " ('Embarked', 'string'),\n",
       " ('Gender', 'double'),\n",
       " ('Boarded', 'double')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----+-------+------+-------+\n",
      "|Survived|Pclass| Age|   Fare|Gender|Boarded|\n",
      "+--------+------+----+-------+------+-------+\n",
      "|     0.0|   3.0|22.0|   7.25|   0.0|    0.0|\n",
      "|     1.0|   1.0|38.0|71.2833|   1.0|    1.0|\n",
      "|     1.0|   3.0|26.0|  7.925|   1.0|    0.0|\n",
      "|     1.0|   1.0|35.0|   53.1|   1.0|    0.0|\n",
      "|     0.0|   3.0|35.0|   8.05|   0.0|    0.0|\n",
      "|     0.0|   1.0|54.0|51.8625|   0.0|    0.0|\n",
      "|     0.0|   3.0| 2.0| 21.075|   0.0|    0.0|\n",
      "|     1.0|   3.0|27.0|11.1333|   1.0|    0.0|\n",
      "|     1.0|   2.0|14.0|30.0708|   1.0|    1.0|\n",
      "|     1.0|   3.0| 4.0|   16.7|   1.0|    0.0|\n",
      "|     1.0|   1.0|58.0|  26.55|   1.0|    0.0|\n",
      "|     0.0|   3.0|20.0|   8.05|   0.0|    0.0|\n",
      "|     0.0|   3.0|39.0| 31.275|   0.0|    0.0|\n",
      "|     0.0|   3.0|14.0| 7.8542|   1.0|    0.0|\n",
      "|     1.0|   2.0|55.0|   16.0|   1.0|    0.0|\n",
      "|     0.0|   3.0| 2.0| 29.125|   0.0|    2.0|\n",
      "|     0.0|   3.0|31.0|   18.0|   1.0|    0.0|\n",
      "|     0.0|   2.0|35.0|   26.0|   0.0|    0.0|\n",
      "|     1.0|   2.0|34.0|   13.0|   0.0|    0.0|\n",
      "|     1.0|   3.0|15.0| 8.0292|   1.0|    2.0|\n",
      "+--------+------+----+-------+------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop unnecessary columns\n",
    "dataset = dataset.drop('Sex')\n",
    "dataset = dataset.drop('Embarked')\n",
    "dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble all the features with VectorAssembler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "required_features = ['Pclass',\n",
    "                    'Age',\n",
    "                    'Fare',\n",
    "                    'Gender',\n",
    "                    'Boarded'\n",
    "                   ]\n",
    "\n",
    "\n",
    "assembler = VectorAssembler(inputCols=required_features, outputCol='features')\n",
    "transformed_data = assembler.transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----+-------+------+-------+--------------------+\n",
      "|Survived|Pclass| Age|   Fare|Gender|Boarded|            features|\n",
      "+--------+------+----+-------+------+-------+--------------------+\n",
      "|     0.0|   3.0|22.0|   7.25|   0.0|    0.0|[3.0,22.0,7.25,0....|\n",
      "|     1.0|   1.0|38.0|71.2833|   1.0|    1.0|[1.0,38.0,71.2833...|\n",
      "|     1.0|   3.0|26.0|  7.925|   1.0|    0.0|[3.0,26.0,7.92500...|\n",
      "|     1.0|   1.0|35.0|   53.1|   1.0|    0.0|[1.0,35.0,53.0999...|\n",
      "|     0.0|   3.0|35.0|   8.05|   0.0|    0.0|[3.0,35.0,8.05000...|\n",
      "|     0.0|   1.0|54.0|51.8625|   0.0|    0.0|[1.0,54.0,51.8624...|\n",
      "|     0.0|   3.0| 2.0| 21.075|   0.0|    0.0|[3.0,2.0,21.07500...|\n",
      "|     1.0|   3.0|27.0|11.1333|   1.0|    0.0|[3.0,27.0,11.1332...|\n",
      "|     1.0|   2.0|14.0|30.0708|   1.0|    1.0|[2.0,14.0,30.0708...|\n",
      "|     1.0|   3.0| 4.0|   16.7|   1.0|    0.0|[3.0,4.0,16.70000...|\n",
      "|     1.0|   1.0|58.0|  26.55|   1.0|    0.0|[1.0,58.0,26.5499...|\n",
      "|     0.0|   3.0|20.0|   8.05|   0.0|    0.0|[3.0,20.0,8.05000...|\n",
      "|     0.0|   3.0|39.0| 31.275|   0.0|    0.0|[3.0,39.0,31.2749...|\n",
      "|     0.0|   3.0|14.0| 7.8542|   1.0|    0.0|[3.0,14.0,7.85419...|\n",
      "|     1.0|   2.0|55.0|   16.0|   1.0|    0.0|[2.0,55.0,16.0,1....|\n",
      "|     0.0|   3.0| 2.0| 29.125|   0.0|    2.0|[3.0,2.0,29.125,0...|\n",
      "|     0.0|   3.0|31.0|   18.0|   1.0|    0.0|[3.0,31.0,18.0,1....|\n",
      "|     0.0|   2.0|35.0|   26.0|   0.0|    0.0|[2.0,35.0,26.0,0....|\n",
      "|     1.0|   2.0|34.0|   13.0|   0.0|    0.0|[2.0,34.0,13.0,0....|\n",
      "|     1.0|   3.0|15.0| 8.0292|   1.0|    2.0|[3.0,15.0,8.02919...|\n",
      "+--------+------+----+-------+------+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Survived=0.0, Pclass=3.0, Age=22.0, Fare=7.25, Gender=0.0, Boarded=0.0, features=DenseVector([3.0, 22.0, 7.25, 0.0, 0.0]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fase 3. Entrenamiento del modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training_data, test_data) = transformed_data.randomSplit([0.8,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(labelCol='Survived', \n",
    "                            featuresCol='features',\n",
    "                            maxDepth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rf.fit(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with the test dataset\n",
    "predictions = model.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fase 4. Evaluación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate our model\n",
    "#from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "#evaluator = BinaryClassificationEvaluator(\n",
    "#    labelCol='Survived', \n",
    "#    predictionCol='prediction', \n",
    "#    metricName='accuracy')\n",
    "\n",
    "# Evaluate our model\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Create an evaluator for binary classification\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='Survived', metricName='areaUnderROC')\n",
    "\n",
    "# Evaluate the model\n",
    "# Note: You need to have a DataFrame with predictions already computed\n",
    "# For example, predictions = model.transform(testData)\n",
    "accuracy = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy =  0.8840613026819925\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print('Test Accuracy = ', accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
