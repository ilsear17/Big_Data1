{"cells":[{"cell_type":"markdown","metadata":{"id":"MDYTiqW5TYZq"},"source":["# Fundamentos de Apache Spark: SQL/DataFrames"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R6Ys7cmsUiGR"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JnPJJXzjUiV5"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"uKgHrskPTYZr"},"source":["**Spark SQLtrabaja con DataFrames**. Un DataFrame es una **representación relacional de los datos**. Proporciona funciones con capacidades similares a SQL. Además, permite escribir **consultas tipo SQL** para nuestro análisis de datos.\n","\n","Los DataFrames son similares a las tablas relacionales o DataFrames en Python / R auqnue con muchas optimizaciones que se ejecutan de manera \"oculta\" para el usuario. Hay varias formas de crear DataFrames a partir de colecciones, tablas HIVE, tablas relacionales y RDD."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"Gt1FjfCRTYZs"},"outputs":[],"source":["import findspark\n","findspark.init()\n","\n","import pandas as pd\n","import pyspark"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"Mo81n3gsTYZs"},"outputs":[],"source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import *"]},{"cell_type":"markdown","metadata":{"id":"1KOjuEjRTYZs"},"source":["### Crear la sesión de Spark"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"A8Eg2IU8TYZs"},"outputs":[],"source":["spark = SparkSession.builder.getOrCreate()"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"3ipWZiQQTYZs","outputId":"1de2e905-7e57-4692-8e05-ebb32d4ad277"},"outputs":[{"data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://DESKTOP-RE9M0KP:4041\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.4.1</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>pyspark-shell</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x1c1df24f1f0>"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["spark"]},{"cell_type":"markdown","metadata":{"id":"SoOWuUF7TYZt"},"source":["### Crear el DataFrame"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"7qm71HY9TYZt"},"outputs":[],"source":["emp = [(1, \"AAA\", \"dept1\", 1000),\n","    (2, \"BBB\", \"dept1\", 1100),\n","    (3, \"CCC\", \"dept1\", 3000),\n","    (4, \"DDD\", \"dept1\", 1500),\n","    (5, \"EEE\", \"dept2\", 8000),\n","    (6, \"FFF\", \"dept2\", 7200),\n","    (7, \"GGG\", \"dept3\", 7100),\n","    (8, \"HHH\", \"dept3\", 3700),\n","    (9, \"III\", \"dept3\", 4500),\n","    (10, \"JJJ\", \"dept5\", 3400)]\n","\n","dept = [(\"dept1\", \"Department - 1\"),\n","        (\"dept2\", \"Department - 2\"),\n","        (\"dept3\", \"Department - 3\"),\n","        (\"dept4\", \"Department - 4\")\n","\n","       ]\n","\n","df = spark.createDataFrame(emp, [\"id\", \"name\", \"dept\", \"salary\"])\n","\n","deptdf = spark.createDataFrame(dept, [\"id\", \"name\"])"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"tI_o0BDOTYZt","outputId":"b373187a-043e-4ad9-a363-49d9edd6da18"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---+----+-----+------+\n","| id|name| dept|salary|\n","+---+----+-----+------+\n","|  1| AAA|dept1|  1000|\n","|  2| BBB|dept1|  1100|\n","|  3| CCC|dept1|  3000|\n","|  4| DDD|dept1|  1500|\n","|  5| EEE|dept2|  8000|\n","|  6| FFF|dept2|  7200|\n","|  7| GGG|dept3|  7100|\n","|  8| HHH|dept3|  3700|\n","|  9| III|dept3|  4500|\n","| 10| JJJ|dept5|  3400|\n","+---+----+-----+------+\n","\n"]}],"source":["df.show()"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"qozsMabSTYZt"},"outputs":[{"ename":"SyntaxError","evalue":"invalid character '“' (U+201C) (3980929210.py, line 2)","output_type":"error","traceback":["\u001b[1;36m  Cell \u001b[1;32mIn[8], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    df = spark.table(“tbl_name”)\u001b[0m\n\u001b[1;37m                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character '“' (U+201C)\n"]}],"source":["#Crear un df a partir de una tabla de Hive\n","df = spark.table(“tbl_name”)"]},{"cell_type":"markdown","metadata":{"id":"sNLrb4NcTYZu"},"source":["# Operaciones básicas en DataFrames"]},{"cell_type":"markdown","metadata":{"id":"wQ70WZBbTYZu"},"source":["### count\n","* Cuenta el número de filas"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"iZrTigoxTYZu","outputId":"6fb7ff47-02e2-48a6-d9b2-55b394ef2a29"},"outputs":[{"data":{"text/plain":["10"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["df.count()"]},{"cell_type":"markdown","metadata":{"id":"KOQXu5nxTYZu"},"source":["### columns"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"tI9aMtf7TYZu","outputId":"370728bf-b511-4d37-a659-87265df30c4e"},"outputs":[{"data":{"text/plain":["['id', 'name', 'dept', 'salary']"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["df.columns"]},{"cell_type":"markdown","metadata":{"id":"KiPhishWTYZv"},"source":["### dtypes\n","** Accede al DataType de columnas dentro del DataFrame"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"h1MH-gHQTYZv","outputId":"983667d2-65d7-40b5-972f-ac689d68108d"},"outputs":[{"data":{"text/plain":["[('id', 'bigint'),\n"," ('name', 'string'),\n"," ('dept', 'string'),\n"," ('salary', 'bigint')]"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["df.dtypes"]},{"cell_type":"markdown","metadata":{"id":"uGCLglj5TYZv"},"source":["### schema\n","** Comprueba cómo Spark almacena el esquema del DataFrame"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"ygU8un90TYZv","outputId":"246059a0-a3f0-4137-f002-9479079ff387"},"outputs":[{"data":{"text/plain":["StructType([StructField('id', LongType(), True), StructField('name', StringType(), True), StructField('dept', StringType(), True), StructField('salary', LongType(), True)])"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["df.schema"]},{"cell_type":"markdown","metadata":{"id":"k648AfQRTYZv"},"source":["### printSchema"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"7jWxQSokTYZv","outputId":"7fd8aa06-e32a-45ff-fbcb-6d2e477c919a"},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- id: long (nullable = true)\n"," |-- name: string (nullable = true)\n"," |-- dept: string (nullable = true)\n"," |-- salary: long (nullable = true)\n","\n"]}],"source":["df.printSchema()"]},{"cell_type":"markdown","metadata":{"id":"tV5Zvf6KTYZv"},"source":["### select\n","* Seleccione columnas del DataFrame"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"lgGff5waTYZv","outputId":"9f08555b-4d98-44df-8f96-4fbba98e0543"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---+----+\n","| id|name|\n","+---+----+\n","|  1| AAA|\n","|  2| BBB|\n","|  3| CCC|\n","|  4| DDD|\n","|  5| EEE|\n","|  6| FFF|\n","|  7| GGG|\n","|  8| HHH|\n","|  9| III|\n","| 10| JJJ|\n","+---+----+\n","\n"]}],"source":["df.select(\"id\", \"name\").show()"]},{"cell_type":"markdown","metadata":{"id":"kDhAw5Z8TYZw"},"source":["### filter\n","\n","* Filtrar las filas según alguna condición.\n","* Intentemos encontrar las filas con id = 1.\n","* Hay diferentes formas de especificar la condición."]},{"cell_type":"code","execution_count":15,"metadata":{"id":"j5IyEeOCTYZw","outputId":"a8d06447-4429-46a5-893b-f01a9203a3eb"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---+----+-----+------+\n","| id|name| dept|salary|\n","+---+----+-----+------+\n","|  1| AAA|dept1|  1000|\n","+---+----+-----+------+\n","\n","+---+----+-----+------+\n","| id|name| dept|salary|\n","+---+----+-----+------+\n","|  1| AAA|dept1|  1000|\n","+---+----+-----+------+\n","\n"]}],"source":["df.filter(df[\"id\"] == 1).show()\n","df.filter(df.id == 1).show()"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"FwtD-w84TYZw","outputId":"f114d5fd-db21-4cae-d92e-93433fabb658"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---+----+-----+------+\n","| id|name| dept|salary|\n","+---+----+-----+------+\n","|  1| AAA|dept1|  1000|\n","+---+----+-----+------+\n","\n","+---+----+-----+------+\n","| id|name| dept|salary|\n","+---+----+-----+------+\n","|  1| AAA|dept1|  1000|\n","+---+----+-----+------+\n","\n"]}],"source":["df.filter(col(\"id\") == 1).show()\n","df.filter(\"id = 1\").show()"]},{"cell_type":"markdown","metadata":{"id":"Xxnqpw7kTYZw"},"source":["### drop\n","* Elimina una columna en particular"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"zVW6nHULTYZw","outputId":"2667d4b1-d8b9-4440-9109-3200e96b03f6"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+-----+------+\n","|name| dept|salary|\n","+----+-----+------+\n","| AAA|dept1|  1000|\n","| BBB|dept1|  1100|\n","+----+-----+------+\n","only showing top 2 rows\n","\n"]}],"source":["newdf = df.drop(\"id\")\n","newdf.show(2)"]},{"cell_type":"markdown","metadata":{"id":"s2zGTJ3kTYZw"},"source":["### Aggregations\n","* Podemos usar la función groupBy para agrupar los datos y luego usar la función \"agg\" para realizar la agregación de datos agrupados."]},{"cell_type":"code","execution_count":18,"metadata":{"id":"JK-f_P2oTYZw","outputId":"6839dc15-a4ad-401c-bbd0-247df546fac5"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----+-----+-----+----+----+------+\n","| dept|count|  sum| max| min|   avg|\n","+-----+-----+-----+----+----+------+\n","|dept1|    4| 6600|3000|1000|1650.0|\n","|dept2|    2|15200|8000|7200|7600.0|\n","|dept3|    3|15300|7100|3700|5100.0|\n","|dept5|    1| 3400|3400|3400|3400.0|\n","+-----+-----+-----+----+----+------+\n","\n"]}],"source":["(df.groupBy(\"dept\")\n","    .agg(\n","        count(\"salary\").alias(\"count\"),\n","        sum(\"salary\").alias(\"sum\"),\n","        max(\"salary\").alias(\"max\"),\n","        min(\"salary\").alias(\"min\"),\n","        avg(\"salary\").alias(\"avg\")\n","        ).show()\n",")"]},{"cell_type":"markdown","metadata":{"id":"xb2i7qfoTYZw"},"source":["### Sorting\n","\n","* Ordena los datos según el \"salario\". De forma predeterminada, la clasificación se realizará en orden ascendente."]},{"cell_type":"code","execution_count":19,"metadata":{"id":"9N3JMX-vTYZx","outputId":"91e3dfcd-8977-4976-f1c5-804f30d0a6c9"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---+----+-----+------+\n","| id|name| dept|salary|\n","+---+----+-----+------+\n","|  1| AAA|dept1|  1000|\n","|  2| BBB|dept1|  1100|\n","|  4| DDD|dept1|  1500|\n","|  3| CCC|dept1|  3000|\n","| 10| JJJ|dept5|  3400|\n","+---+----+-----+------+\n","only showing top 5 rows\n","\n"]}],"source":["df.sort(\"salary\").show(5)"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"dXipIftPTYZx","outputId":"81fceb64-bc9a-4d0b-d04c-b0bbc19c1c1e"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---+----+-----+------+\n","| id|name| dept|salary|\n","+---+----+-----+------+\n","|  5| EEE|dept2|  8000|\n","|  6| FFF|dept2|  7200|\n","|  7| GGG|dept3|  7100|\n","|  9| III|dept3|  4500|\n","|  8| HHH|dept3|  3700|\n","+---+----+-----+------+\n","only showing top 5 rows\n","\n"]}],"source":["# Sort the data in descending order.\n","df.sort(desc(\"salary\")).show(5)"]},{"cell_type":"markdown","metadata":{"id":"nz5VRkd5TYZx"},"source":["### Columnas derivadas\n","* Podemos usar la función \"withColumn\" para derivar la columna en función de las columnas existentes ..."]},{"cell_type":"code","execution_count":21,"metadata":{"id":"9xfk-En4TYZx","outputId":"037f8f32-cc74-4ffc-86db-e8558d19d177"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---+----+-----+------+-----+\n","| id|name| dept|salary|bonus|\n","+---+----+-----+------+-----+\n","|  1| AAA|dept1|  1000|100.0|\n","|  2| BBB|dept1|  1100|110.0|\n","|  3| CCC|dept1|  3000|300.0|\n","|  4| DDD|dept1|  1500|150.0|\n","|  5| EEE|dept2|  8000|800.0|\n","|  6| FFF|dept2|  7200|720.0|\n","|  7| GGG|dept3|  7100|710.0|\n","|  8| HHH|dept3|  3700|370.0|\n","|  9| III|dept3|  4500|450.0|\n","| 10| JJJ|dept5|  3400|340.0|\n","+---+----+-----+------+-----+\n","\n"]}],"source":["df.withColumn(\"bonus\", col(\"salary\") * .1).show()"]},{"cell_type":"markdown","metadata":{"id":"-KlAQl2eTYZx"},"source":["### Joins\n","\n","* Podemos realizar varios tipos de combinaciones en múltiples DataFrames."]},{"cell_type":"markdown","metadata":{"id":"FKq2jMSqTYZx"},"source":["![Sin%20t%C3%ADtulo.png](attachment:Sin%20t%C3%ADtulo.png)"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"hC9PPrsbTYZx","outputId":"a78feb7b-8338-45e6-bc15-0f1140a24316"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---+----+-----+------+-----+--------------+\n","| id|name| dept|salary|   id|          name|\n","+---+----+-----+------+-----+--------------+\n","|  1| AAA|dept1|  1000|dept1|Department - 1|\n","|  2| BBB|dept1|  1100|dept1|Department - 1|\n","|  3| CCC|dept1|  3000|dept1|Department - 1|\n","|  4| DDD|dept1|  1500|dept1|Department - 1|\n","|  5| EEE|dept2|  8000|dept2|Department - 2|\n","|  6| FFF|dept2|  7200|dept2|Department - 2|\n","|  7| GGG|dept3|  7100|dept3|Department - 3|\n","|  8| HHH|dept3|  3700|dept3|Department - 3|\n","|  9| III|dept3|  4500|dept3|Department - 3|\n","+---+----+-----+------+-----+--------------+\n","\n"]}],"source":["# Inner JOIN.\n","df.join(deptdf, df[\"dept\"] == deptdf[\"id\"]).show()"]},{"cell_type":"markdown","metadata":{"id":"5pMypSwZTYZy"},"source":["### Left Outer Join"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"bxD9bnxJTYZy","outputId":"985000f8-8d9e-4da2-fd09-ddc5c7b82887"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---+----+-----+------+-----+--------------+\n","| id|name| dept|salary|   id|          name|\n","+---+----+-----+------+-----+--------------+\n","|  1| AAA|dept1|  1000|dept1|Department - 1|\n","|  2| BBB|dept1|  1100|dept1|Department - 1|\n","|  3| CCC|dept1|  3000|dept1|Department - 1|\n","|  4| DDD|dept1|  1500|dept1|Department - 1|\n","|  5| EEE|dept2|  8000|dept2|Department - 2|\n","|  6| FFF|dept2|  7200|dept2|Department - 2|\n","|  7| GGG|dept3|  7100|dept3|Department - 3|\n","|  8| HHH|dept3|  3700|dept3|Department - 3|\n","| 10| JJJ|dept5|  3400| null|          null|\n","|  9| III|dept3|  4500|dept3|Department - 3|\n","+---+----+-----+------+-----+--------------+\n","\n"]}],"source":["df.join(deptdf, df[\"dept\"] == deptdf[\"id\"], \"left_outer\").show()"]},{"cell_type":"markdown","metadata":{"id":"OxWZw-SOTYZy"},"source":["### Right Outer Join"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"aMf7blwmTYZy","outputId":"c88c16e0-20ea-4e81-f563-b9f1b717cd35"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+----+-----+------+-----+--------------+\n","|  id|name| dept|salary|   id|          name|\n","+----+----+-----+------+-----+--------------+\n","|   4| DDD|dept1|  1500|dept1|Department - 1|\n","|   3| CCC|dept1|  3000|dept1|Department - 1|\n","|   2| BBB|dept1|  1100|dept1|Department - 1|\n","|   1| AAA|dept1|  1000|dept1|Department - 1|\n","|   6| FFF|dept2|  7200|dept2|Department - 2|\n","|   5| EEE|dept2|  8000|dept2|Department - 2|\n","|   9| III|dept3|  4500|dept3|Department - 3|\n","|   8| HHH|dept3|  3700|dept3|Department - 3|\n","|   7| GGG|dept3|  7100|dept3|Department - 3|\n","|null|null| null|  null|dept4|Department - 4|\n","+----+----+-----+------+-----+--------------+\n","\n"]}],"source":["df.join(deptdf, df[\"dept\"] == deptdf[\"id\"], \"right_outer\").show()"]},{"cell_type":"markdown","metadata":{"id":"yn_WKdj7TYZy"},"source":["### Full Outer Join"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"cyqhcpJmTYZy","outputId":"97a74961-e9e5-4736-d90d-9b0ee2c7cfce"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+----+-----+------+-----+--------------+\n","|  id|name| dept|salary|   id|          name|\n","+----+----+-----+------+-----+--------------+\n","|   1| AAA|dept1|  1000|dept1|Department - 1|\n","|   2| BBB|dept1|  1100|dept1|Department - 1|\n","|   3| CCC|dept1|  3000|dept1|Department - 1|\n","|   4| DDD|dept1|  1500|dept1|Department - 1|\n","|   5| EEE|dept2|  8000|dept2|Department - 2|\n","|   6| FFF|dept2|  7200|dept2|Department - 2|\n","|   7| GGG|dept3|  7100|dept3|Department - 3|\n","|   8| HHH|dept3|  3700|dept3|Department - 3|\n","|   9| III|dept3|  4500|dept3|Department - 3|\n","|null|null| null|  null|dept4|Department - 4|\n","|  10| JJJ|dept5|  3400| null|          null|\n","+----+----+-----+------+-----+--------------+\n","\n"]}],"source":["df.join(deptdf, df[\"dept\"] == deptdf[\"id\"], \"outer\").show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ut-_qwzjTYZz"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"QqlMQsMATYZz"},"source":["### Consultas SQL\n","* Ejecución de consultas tipo SQL.\n","* También podemos realizar análisis de datos escribiendo consultas similares a SQL. Para realizar consultas similares a SQL, necesitamos registrar el DataFrame como una Vista temporal."]},{"cell_type":"code","execution_count":26,"metadata":{"id":"Bdrk9VeATYZz","outputId":"c720cc84-fc35-4b65-a554-0779c2924e07"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---+----+-----+------+\n","| id|name| dept|salary|\n","+---+----+-----+------+\n","|  1| AAA|dept1|  1000|\n","+---+----+-----+------+\n","\n"]}],"source":["# Register DataFrame as Temporary Table\n","df.createOrReplaceTempView(\"temp_table\")\n","\n","# Execute SQL-Like query.\n","spark.sql(\"select * from temp_table where id = 1\").show()"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"2JXyF5ncTYZz","outputId":"232f917c-c11e-4f8c-ec14-986ab0d0fd3f"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---+\n","| id|\n","+---+\n","|  1|\n","|  2|\n","|  3|\n","|  5|\n","|  4|\n","|  6|\n","|  7|\n","|  8|\n","|  9|\n","| 10|\n","+---+\n","\n"]}],"source":["spark.sql(\"select distinct id from temp_table\").show(10)"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"euaWQGqZTYZz","outputId":"0bdff5c6-88c8-495c-d999-8adb41bd8534"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---+----+-----+------+\n","| id|name| dept|salary|\n","+---+----+-----+------+\n","|  3| CCC|dept1|  3000|\n","|  4| DDD|dept1|  1500|\n","|  5| EEE|dept2|  8000|\n","|  6| FFF|dept2|  7200|\n","|  7| GGG|dept3|  7100|\n","|  8| HHH|dept3|  3700|\n","|  9| III|dept3|  4500|\n","| 10| JJJ|dept5|  3400|\n","+---+----+-----+------+\n","\n"]}],"source":["spark.sql(\"select * from temp_table where salary >= 1500\").show(10)"]},{"cell_type":"markdown","metadata":{"id":"GJtP2IZlTYZz"},"source":["### Leyendo la tabla HIVE como DataFrame"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"t6Jq9GqETYZz"},"outputs":[{"ename":"SyntaxError","evalue":"invalid syntax (3900709972.py, line 5)","output_type":"error","traceback":["\u001b[1;36m  Cell \u001b[1;32mIn[29], line 5\u001b[1;36m\u001b[0m\n\u001b[1;33m    df = spark.table(\"DB_NAME\".\"TBL_NAME\")\u001b[0m\n\u001b[1;37m                               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"]}],"source":["# DB_NAME : Name of the the HIVE Database\n","# TBL_NAME : Name of the HIVE Table\n","\n","\n","df = spark.table(\"DB_NAME\".\"TBL_NAME\")"]},{"cell_type":"markdown","metadata":{"id":"j6TI_II8TYZz"},"source":["### Guardar DataFrame como tabla HIVE"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"qNgnXH8KTYZ0"},"outputs":[{"ename":"Py4JJavaError","evalue":"An error occurred while calling o141.saveAsTable.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.liftedTree1$1(InMemoryCatalog.scala:122)\r\n\tat org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.createDatabase(InMemoryCatalog.scala:119)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:159)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:140)\r\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$catalog$1(BaseSessionStateBuilder.scala:154)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog$lzycompute(SessionCatalog.scala:122)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog(SessionCatalog.scala:122)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.tableExists(SessionCatalog.scala:514)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:640)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:570)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:341)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:331)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:370)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:955)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:192)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:215)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1111)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 23 more\r\n","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","\u001b[1;32mc:\\Users\\ilse-\\BIG DATA EJERCICIOS\\2. Fundamentos de Spark_SQL_DataFrames.ipynb Cell 56\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ilse-/BIG%20DATA%20EJERCICIOS/2.%20Fundamentos%20de%20Spark_SQL_DataFrames.ipynb#Y106sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df\u001b[39m.\u001b[39;49mwrite\u001b[39m.\u001b[39;49msaveAsTable(\u001b[39m\"\u001b[39;49m\u001b[39mDB_NAME.TBL_NAME\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ilse-/BIG%20DATA%20EJERCICIOS/2.%20Fundamentos%20de%20Spark_SQL_DataFrames.ipynb#Y106sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m## También podemos seleccionar el argumento \"modo\" con overwrite\", \"append\", \"error\" etc.\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ilse-/BIG%20DATA%20EJERCICIOS/2.%20Fundamentos%20de%20Spark_SQL_DataFrames.ipynb#Y106sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m df\u001b[39m.\u001b[39mwrite\u001b[39m.\u001b[39msaveAsTable(\u001b[39m\"\u001b[39m\u001b[39mDB_NAME.TBL_NAME\u001b[39m\u001b[39m\"\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39moverwrite\u001b[39m\u001b[39m\"\u001b[39m)\n","File \u001b[1;32mc:\\Users\\ilse-\\anaconda3\\envs\\bigdata\\lib\\site-packages\\pyspark\\sql\\readwriter.py:1521\u001b[0m, in \u001b[0;36mDataFrameWriter.saveAsTable\u001b[1;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m   1519\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mformat\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1520\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mformat\u001b[39m)\n\u001b[1;32m-> 1521\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49msaveAsTable(name)\n","File \u001b[1;32mc:\\Users\\ilse-\\anaconda3\\envs\\bigdata\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n","File \u001b[1;32mc:\\Users\\ilse-\\anaconda3\\envs\\bigdata\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    170\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n","File \u001b[1;32mc:\\Users\\ilse-\\anaconda3\\envs\\bigdata\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n","\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o141.saveAsTable.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.liftedTree1$1(InMemoryCatalog.scala:122)\r\n\tat org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.createDatabase(InMemoryCatalog.scala:119)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:159)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:140)\r\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$catalog$1(BaseSessionStateBuilder.scala:154)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog$lzycompute(SessionCatalog.scala:122)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog(SessionCatalog.scala:122)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.tableExists(SessionCatalog.scala:514)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:640)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:570)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:341)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:331)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:370)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:955)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:192)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:215)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1111)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 23 more\r\n"]}],"source":["df.write.saveAsTable(\"DB_NAME.TBL_NAME\")\n","\n","## También podemos seleccionar el argumento \"modo\" con overwrite\", \"append\", \"error\" etc.\n","df.write.saveAsTable(\"DB_NAME.TBL_NAME\", mode=\"overwrite\")\n","\n","# De forma predeterminada, la operación guardará el DataFrame como una tabla interna / administrada de HIVE"]},{"cell_type":"markdown","metadata":{"id":"858Nw5WQTYZ0"},"source":["### Guardar el DataFrame como una tabla externa HIVE"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"qcXaQuQhTYZ0"},"outputs":[{"ename":"SyntaxError","evalue":"invalid syntax (3742179535.py, line 1)","output_type":"error","traceback":["\u001b[1;36m  Cell \u001b[1;32mIn[32], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    df.write.saveAsTable(\"DB_NAME.TBL_NAME\", path=<location_of_external_table>)\u001b[0m\n\u001b[1;37m                                                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"]}],"source":["df.write.saveAsTable(\"DB_NAME.TBL_NAME\", path=<location_of_external_table>)"]},{"cell_type":"markdown","metadata":{"id":"VYxk9Ld6TYZ0"},"source":["### Crea un DataFrame a partir de un archivo CSV\n","* Podemos crear un DataFrame usando un archivo CSV y podemos especificar varias opciones como un separador, encabezado, esquema, inferSchema y varias otras opciones."]},{"cell_type":"code","execution_count":33,"metadata":{"id":"QcCis18jTYZ0"},"outputs":[{"ename":"AnalysisException","evalue":"[PATH_NOT_FOUND] Path does not exist: file:/c:/Users/ilse-/BIG DATA EJERCICIOS/path_to_csv_file.","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)","\u001b[1;32mc:\\Users\\ilse-\\BIG DATA EJERCICIOS\\2. Fundamentos de Spark_SQL_DataFrames.ipynb Cell 60\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ilse-/BIG%20DATA%20EJERCICIOS/2.%20Fundamentos%20de%20Spark_SQL_DataFrames.ipynb#Y113sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39;49mread\u001b[39m.\u001b[39;49mcsv(\u001b[39m\"\u001b[39;49m\u001b[39mpath_to_csv_file\u001b[39;49m\u001b[39m\"\u001b[39;49m, sep\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m|\u001b[39;49m\u001b[39m\"\u001b[39;49m, header\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, inferSchema\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n","File \u001b[1;32mc:\\Users\\ilse-\\anaconda3\\envs\\bigdata\\lib\\site-packages\\pyspark\\sql\\readwriter.py:727\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[1;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[0;32m    725\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(path) \u001b[39m==\u001b[39m \u001b[39mlist\u001b[39m:\n\u001b[0;32m    726\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_spark\u001b[39m.\u001b[39m_sc\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 727\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_df(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jreader\u001b[39m.\u001b[39;49mcsv(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_spark\u001b[39m.\u001b[39;49m_sc\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mPythonUtils\u001b[39m.\u001b[39;49mtoSeq(path)))\n\u001b[0;32m    728\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(path, RDD):\n\u001b[0;32m    730\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mfunc\u001b[39m(iterator):\n","File \u001b[1;32mc:\\Users\\ilse-\\anaconda3\\envs\\bigdata\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n","File \u001b[1;32mc:\\Users\\ilse-\\anaconda3\\envs\\bigdata\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    171\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[0;32m    172\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    173\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    174\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[39mraise\u001b[39;00m\n","\u001b[1;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/c:/Users/ilse-/BIG DATA EJERCICIOS/path_to_csv_file."]}],"source":["df = spark.read.csv(\"path_to_csv_file\", sep=\"|\", header=True, inferSchema=True)"]},{"cell_type":"markdown","metadata":{"id":"l87u7lneTYZ0"},"source":["### Guardar un DataFrame como un archivo CSV"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"V8fO1lAhTYZ0"},"outputs":[{"ename":"Py4JJavaError","evalue":"An error occurred while calling o158.csv.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:847)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:341)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:331)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:370)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:955)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:192)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:215)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1111)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 23 more\r\n","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","\u001b[1;32mc:\\Users\\ilse-\\BIG DATA EJERCICIOS\\2. Fundamentos de Spark_SQL_DataFrames.ipynb Cell 62\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ilse-/BIG%20DATA%20EJERCICIOS/2.%20Fundamentos%20de%20Spark_SQL_DataFrames.ipynb#Y115sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df\u001b[39m.\u001b[39;49mwrite\u001b[39m.\u001b[39;49mcsv(\u001b[39m\"\u001b[39;49m\u001b[39mpath_to_CSV_File\u001b[39;49m\u001b[39m\"\u001b[39;49m, sep\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m|\u001b[39;49m\u001b[39m\"\u001b[39;49m, header\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39moverwrite\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n","File \u001b[1;32mc:\\Users\\ilse-\\anaconda3\\envs\\bigdata\\lib\\site-packages\\pyspark\\sql\\readwriter.py:1799\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[0;32m   1780\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode(mode)\n\u001b[0;32m   1781\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_opts(\n\u001b[0;32m   1782\u001b[0m     compression\u001b[39m=\u001b[39mcompression,\n\u001b[0;32m   1783\u001b[0m     sep\u001b[39m=\u001b[39msep,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1797\u001b[0m     lineSep\u001b[39m=\u001b[39mlineSep,\n\u001b[0;32m   1798\u001b[0m )\n\u001b[1;32m-> 1799\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49mcsv(path)\n","File \u001b[1;32mc:\\Users\\ilse-\\anaconda3\\envs\\bigdata\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n","File \u001b[1;32mc:\\Users\\ilse-\\anaconda3\\envs\\bigdata\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    170\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n","File \u001b[1;32mc:\\Users\\ilse-\\anaconda3\\envs\\bigdata\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n","\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o158.csv.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:847)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:341)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:331)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:370)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:955)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:192)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:215)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1111)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 23 more\r\n"]}],"source":["df.write.csv(\"path_to_CSV_File\", sep=\"|\", header=True, mode=\"overwrite\")"]},{"cell_type":"markdown","metadata":{"id":"oOCuAEobTYZ0"},"source":["### Crea un DataFrame a partir de una tabla relacional\n","* Podemos leer los datos de bases de datos relacionales usando una URL JDBC."]},{"cell_type":"code","execution_count":35,"metadata":{"id":"mpHAlInPTYZ0"},"outputs":[{"ename":"IndentationError","evalue":"unexpected indent (1561999250.py, line 8)","output_type":"error","traceback":["\u001b[1;36m  Cell \u001b[1;32mIn[35], line 8\u001b[1;36m\u001b[0m\n\u001b[1;33m    .options(url=url, dbtable= <TBL_NAME>, user= <USER_NAME>, password = <PASSWORD>)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"]}],"source":["# url : a JDBC URL of the form jdbc:subprotocol:subname\n","# TBL_NAME : Name of the relational table.\n","# USER_NAME : user name to connect to DataBase.\n","# PASSWORD: password to connect to DataBase.\n","\n","\n","relational_df = spark.read.format('jdbc')\n","                        .options(url=url, dbtable= <TBL_NAME>, user= <USER_NAME>, password = <PASSWORD>)\n","                        .load()"]},{"cell_type":"markdown","metadata":{"id":"H1JH_PahTYZ1"},"source":["### Guardar el DataFrame como una tabla relacional\n","* Podemos guardar el DataFrame como una tabla relacional usando una URL JDBC."]},{"cell_type":"code","execution_count":36,"metadata":{"id":"dj3G9Un8TYZ1"},"outputs":[{"ename":"IndentationError","evalue":"unexpected indent (3038840136.py, line 7)","output_type":"error","traceback":["\u001b[1;36m  Cell \u001b[1;32mIn[36], line 7\u001b[1;36m\u001b[0m\n\u001b[1;33m    relational_df.write.format('jdbc')\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"]}],"source":["# url : a JDBC URL of the form jdbc:subprotocol:subname\n","# TBL_NAME : Name of the relational table.\n","# USER_NAME : user name to connect to DataBase.\n","# PASSWORD: password to connect to DataBase.\n","\n","\n"," relational_df.write.format('jdbc')\n","                    .options(url=url, dbtable= <TBL_NAME>, user= <USER_NAME>, password = <PASSWORD>)\n","                    .mode('overwrite')\n","                    .save()"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
