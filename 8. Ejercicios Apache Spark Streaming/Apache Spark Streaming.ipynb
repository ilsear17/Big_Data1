{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caso práctico con  Spark Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este noteboo se utilizarán los datos de Kaggle de fraud detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext nb_black\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"C:/Users/ilse-/fraud_detection.csv\", \n",
    "                    header=True, \n",
    "                    inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['step',\n",
       " 'type',\n",
       " 'amount',\n",
       " 'nameOrig',\n",
       " 'oldbalanceOrg',\n",
       " 'newbalanceOrig',\n",
       " 'nameDest',\n",
       " 'oldbalanceDest',\n",
       " 'newbalanceDest',\n",
       " 'isFraud',\n",
       " 'isFlaggedFraud']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"isFraud\", \"isFlaggedFraud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-------+-----------+-------------+--------------+-----------+--------------+--------------+\n",
      "|step|   type| amount|   nameOrig|oldbalanceOrg|newbalanceOrig|   nameDest|oldbalanceDest|newbalanceDest|\n",
      "+----+-------+-------+-----------+-------------+--------------+-----------+--------------+--------------+\n",
      "|   1|PAYMENT|9839.64|C1231006815|     170136.0|     160296.36|M1979787155|           0.0|           0.0|\n",
      "|   1|PAYMENT|1864.28|C1666544295|      21249.0|      19384.72|M2044282225|           0.0|           0.0|\n",
      "+----+-------+-------+-----------+-------------+--------------+-----------+--------------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step maps a unit of time in the real world. In this case 1 step is 1 hour of time. So we can assume for this example that we have another job that runs every hour and gets all the transactions in that time frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|step|count|\n",
      "+----+-----+\n",
      "| 148|   12|\n",
      "| 463|   10|\n",
      "| 471| 2620|\n",
      "+----+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"step\").count().show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can therefore save the output of that job by filtering on each step and saving it to a separate file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'steps = df.select(\"step\").distinct().collect()\\nfor step in steps[:]:\\n    _df = df.where(f\"step = {step[0]}\")\\n    #by adding coalesce(1) we save the dataframe to one file\\n    _df.coalesce(1).write.mode(\"append\").option(\"header\", \"true\").csv(\"C:/Users/ilse-/fraud_detection.csv\")'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"steps = df.select(\"step\").distinct().collect()\n",
    "for step in steps[:]:\n",
    "    _df = df.where(f\"step = {step[0]}\")\n",
    "    #by adding coalesce(1) we save the dataframe to one file\n",
    "    _df.coalesce(1).write.mode(\"append\").option(\"header\", \"true\").csv(\"C:/Users/ilse-/fraud_detection.csv\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "El sistema no puede encontrar la ruta especificada.\n"
     ]
    }
   ],
   "source": [
    "!cd data/fraud "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "part = spark.read.csv(\n",
    "    \"C:/Users/ilse-/fraud_detection.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|step|count|\n",
      "+----+-----+\n",
      "| 148|   12|\n",
      "| 463|   10|\n",
      "| 471| 2620|\n",
      "| 496|  873|\n",
      "| 243|    8|\n",
      "| 392|   10|\n",
      "| 540| 5476|\n",
      "| 623|    6|\n",
      "| 737|   10|\n",
      "|  31|   12|\n",
      "| 516|   14|\n",
      "|  85|   14|\n",
      "| 137|32559|\n",
      "| 251|35269|\n",
      "| 451| 3751|\n",
      "| 580|   58|\n",
      "|  65|   20|\n",
      "| 458|  438|\n",
      "|  53|   10|\n",
      "| 255|28840|\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "part.groupBy(\"step\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s create a streaming version of this input, we'll read each file one by one as if it was a stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSchema = part.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(step,IntegerType,true),StructField(type,StringType,true),StructField(amount,DoubleType,true),StructField(nameOrig,StringType,true),StructField(oldbalanceOrg,DoubleType,true),StructField(newbalanceOrig,DoubleType,true),StructField(nameDest,StringType,true),StructField(oldbalanceDest,DoubleType,true),StructField(newbalanceDest,DoubleType,true),StructField(isFraud,IntegerType,true),StructField(isFlaggedFraud,IntegerType,true)))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*maxFilesPerTrigger* allows you to control how quickly Spark will read all of the files in the folder. \n",
    "In this example we're limiting the flow of the stream to one file per trigger.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming = (\n",
    "    spark.readStream.schema(dataSchema)\n",
    "    .option(\"maxFilesPerTrigger\", 1)\n",
    "    .csv(\"C:/Users/ilse-/fraud\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up a transformation.\n",
    "\n",
    "The nameDest column is the recipient ID of the transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_count = streaming.groupBy(\"nameDest\").count().orderBy(F.desc(\"count\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our transformation, we need to specify an output sink for the results. For this example, we're going to write to a memory sink which keeps the results in memory.\n",
    "\n",
    "We also need to define how Spark will output that data. In this example, we'll use the complete output mode (rewriting all of the keys along with their counts after every trigger).\n",
    "\n",
    "In this example we won't include activityQuery.awaitTermination() because it is required only to prevent the driver process from terminating when the stream is active.\n",
    "\n",
    "So in order to be able to run this locally in a notebook we won't include it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: unknown command \"install-c\" - maybe you meant \"install\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install-c cyclus java-jdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-36-76b268aa55f2>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-36-76b268aa55f2>\"\u001b[1;36m, line \u001b[1;32m8\u001b[0m\n\u001b[1;33m    include this in production\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "activityQuery = (\n",
    "    dest_count.writeStream.queryName(\"dest_counts\")\n",
    "    .format(\"memory\")\n",
    "    .outputMode(\"complete\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "include this in production\n",
    "activityQuery.awaitTermination()\n",
    "\n",
    "import time\n",
    "\n",
    "for x in range(50):\n",
    "    _df = spark.sql(\n",
    "        \"SELECT * FROM dest_counts WHERE nameDest != 'nameDest' AND count >= 2\"\n",
    "    )\n",
    "    if _df.count() > 0:\n",
    "        _df.show(10)\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if stream is active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-dc071646db1b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstreams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactive\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misActive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "spark.streams.active[0].isActive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'activityQuery' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-d06a57f8ac88>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mactivityQuery\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'activityQuery' is not defined"
     ]
    }
   ],
   "source": [
    "activityQuery.status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we  want to turn off the stream we'll run activityQuery.stop() to reset the query for testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'activityQuery' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-300afb79bec9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mactivityQuery\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'activityQuery' is not defined"
     ]
    }
   ],
   "source": [
    "activityQuery.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
