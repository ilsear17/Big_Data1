{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9ba9a31f",
      "metadata": {
        "id": "9ba9a31f"
      },
      "source": [
        "# IlseArredondo.323019078.Ejercicio_Predicción en Streaming con Spark ML y Spark Streaming"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80ea30cb",
      "metadata": {
        "id": "80ea30cb"
      },
      "source": [
        "En este notebook vamos a cargar un pipeline que tiene un conjunto de fases de pre-procesamiento y un modelo de clasificacion predecir la probabilidad de un paciente de sufrir un ataque al corazón. La predicción se realizará sobre datos en streaming optenidos a partir del csv de heart.csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "!pip install findspark\n",
        "!pip install koalas\n",
        "!pip install plotly\n",
        "!pip install nbformat\n",
        "!pip install databricks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYD0sPRaY8Ee",
        "outputId": "9e8e5a3f-4bb0-45d2-f13f-90b500de3b83"
      },
      "id": "vYD0sPRaY8Ee",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425345 sha256=88ac7d9d371be76e7549a61c8bd1f0bd0cf6eb6f220151e1656cf332a5101ce8\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n",
            "Collecting koalas\n",
            "  Downloading koalas-0.32.0-py3-none-any.whl (593 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m593.2/593.2 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from koalas) (1.5.3)\n",
            "Requirement already satisfied: pyarrow>=0.10 in /usr/local/lib/python3.10/dist-packages (from koalas) (9.0.0)\n",
            "Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.10/dist-packages (from koalas) (1.23.5)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from koalas) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->koalas) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->koalas) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->koalas) (4.45.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->koalas) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->koalas) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->koalas) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->koalas) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->koalas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.2->koalas) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->koalas) (1.16.0)\n",
            "Installing collected packages: koalas\n",
            "Successfully installed koalas-0.32.0\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (5.15.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly) (8.2.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly) (23.2)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (5.9.2)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat) (2.19.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat) (4.19.2)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.10/dist-packages (from nbformat) (5.5.0)\n",
            "Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.10/dist-packages (from nbformat) (5.7.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat) (2023.11.2)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat) (0.31.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat) (0.13.2)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core->nbformat) (4.0.0)\n",
            "Collecting databricks\n",
            "  Downloading databricks-0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Installing collected packages: databricks\n",
            "Successfully installed databricks-0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6318a507",
      "metadata": {
        "id": "6318a507"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "4878544d",
      "metadata": {
        "id": "4878544d"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import OneHotEncoder\n",
        "from pyspark.ml.feature import MinMaxScaler\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.feature import OneHotEncoder\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.sql.types import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "56cecab8",
      "metadata": {
        "id": "56cecab8"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName('UCI Heart disease').getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import pandas as pd\n",
        "uploaded = files.upload()\n",
        "\n",
        "import io\n",
        "df2 = pd.read_csv(io.BytesIO(uploaded['heart.csv']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "U2zxFsC1Zwl8",
        "outputId": "14e42bd9-6d5e-4259-d264-a2abfe2d8c58"
      },
      "id": "U2zxFsC1Zwl8",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ab2854c5-ce5d-455d-a77b-f23e72fe0f69\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ab2854c5-ce5d-455d-a77b-f23e72fe0f69\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving heart.csv to heart.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Carga y visualiza el csv de Ejercicios\\data\\heart.csv con el nombre de heart\n",
        "heart = spark.read.csv('./heart.csv',\n",
        "                       inferSchema = True,\n",
        "                       header = True)"
      ],
      "metadata": {
        "id": "_Xna4gcTaELz"
      },
      "id": "_Xna4gcTaELz",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6qcGXbDwZxyz"
      },
      "outputs": [],
      "source": [
        "schema = StructType( \\\n",
        "                     [StructField(\"age\", LongType(),True), \\\n",
        "                      StructField(\"sex\", LongType(), True), \\\n",
        "                      StructField(\"cp\", LongType(), True), \\\n",
        "                      StructField('trestbps', LongType(), True), \\\n",
        "                      StructField(\"chol\", LongType(), True), \\\n",
        "                      StructField(\"fbs\", LongType(), True), \\\n",
        "                      StructField(\"restecg\", LongType(), True), \\\n",
        "                      StructField(\"thalach\", LongType(), True),\\\n",
        "                      StructField(\"exang\", LongType(), True), \\\n",
        "                      StructField(\"oldpeak\", DoubleType(), True), \\\n",
        "                      StructField(\"slope\", LongType(),True), \\\n",
        "                      StructField(\"ca\", LongType(), True), \\\n",
        "                      StructField(\"thal\", LongType(), True), \\\n",
        "                      StructField(\"target\", LongType(), True), \\\n",
        "                        ])"
      ],
      "id": "6qcGXbDwZxyz"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "28d8812f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28d8812f",
        "outputId": "ca2a795b-b37c-41b8-f7a5-b5162515b547"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- age: integer (nullable = true)\n",
            " |-- sex: integer (nullable = true)\n",
            " |-- cp: integer (nullable = true)\n",
            " |-- trestbps: integer (nullable = true)\n",
            " |-- chol: integer (nullable = true)\n",
            " |-- fbs: integer (nullable = true)\n",
            " |-- restecg: integer (nullable = true)\n",
            " |-- thalach: integer (nullable = true)\n",
            " |-- exang: integer (nullable = true)\n",
            " |-- oldpeak: double (nullable = true)\n",
            " |-- slope: integer (nullable = true)\n",
            " |-- ca: integer (nullable = true)\n",
            " |-- thal: integer (nullable = true)\n",
            " |-- label: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql.types import StructType,StructField,LongType, StringType,DoubleType,TimestampType\n",
        "\n",
        "df = heart.withColumnRenamed(\"target\",\"label\")\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "1b2c3b4b",
      "metadata": {
        "id": "1b2c3b4b"
      },
      "outputs": [],
      "source": [
        "testDF, trainDF = df.randomSplit([0.3, 0.7])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "730KJZWeYt60"
      },
      "source": [
        "## Carga del Pipeline"
      ],
      "id": "730KJZWeYt60"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "9ea2376e",
      "metadata": {
        "id": "9ea2376e"
      },
      "outputs": [],
      "source": [
        "# Create the logistic regression model\n",
        "lr = LogisticRegression(maxIter=10, regParam= 0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "_H3QFIq9Yt61"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml import PipelineModel\n",
        "\n",
        "# Create the logistic regression model\n",
        "lr = LogisticRegression(maxIter=10, regParam= 0.01)\n",
        "\n",
        "# We create a one hot encoder.\n",
        "ohe = OneHotEncoder(inputCols = ['sex', 'cp', 'fbs', 'restecg', 'slope',\n",
        "                                 'exang', 'ca', 'thal'],\n",
        "                    outputCols=['sex_ohe', 'cp_ohe', 'fbs_ohe',\n",
        "                                'restecg_ohe', 'slp_ohe', 'exng_ohe',\n",
        "                                'caa_ohe', 'thall_ohe'])\n",
        "\n",
        "# Input list for scaling\n",
        "inputs = ['age','trestbps','chol','thalach','oldpeak']\n",
        "\n",
        "# We scale our inputs\n",
        "assembler1 = VectorAssembler(inputCols=inputs, outputCol=\"features_scaled1\")\n",
        "scaler = MinMaxScaler(inputCol=\"features_scaled1\", outputCol=\"features_scaled\")\n",
        "\n",
        "# We create a second assembler for the encoded columns.\n",
        "assembler2 = VectorAssembler(inputCols=['sex_ohe', 'cp_ohe',\n",
        "                                        'fbs_ohe', 'restecg_ohe',\n",
        "                                        'slp_ohe', 'exng_ohe', 'caa_ohe',\n",
        "                                        'thall_ohe','features_scaled'],\n",
        "                             outputCol=\"features\")\n",
        "# Create stages list\n",
        "myStages = [assembler1, scaler, ohe, assembler2,lr]\n",
        "# Set up the pipeline\n",
        "pipeline = Pipeline(stages= myStages)\n",
        "# We fit the model using the training data.\n",
        "pModel = pipeline.fit(trainDF)"
      ],
      "id": "_H3QFIq9Yt61"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYInpJ1kYt62",
        "outputId": "5e8528b8-f1f0-4c34-c51a-1ebfdf1a4a13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+----------+\n",
            "|label|         probability|prediction|\n",
            "+-----+--------------------+----------+\n",
            "|    1|[0.06600538313039...|       1.0|\n",
            "|    1|[0.07316247001796...|       1.0|\n",
            "|    0|[0.91713077651849...|       0.0|\n",
            "|    0|[0.76673568590232...|       0.0|\n",
            "|    1|[0.03516086209690...|       1.0|\n",
            "|    1|[0.00557640273219...|       1.0|\n",
            "|    1|[0.06443589101584...|       1.0|\n",
            "|    1|[0.04437533265611...|       1.0|\n",
            "|    1|[0.00388373733811...|       1.0|\n",
            "|    1|[0.02531489565237...|       1.0|\n",
            "|    0|[0.74862356883628...|       0.0|\n",
            "|    0|[0.94566371169472...|       0.0|\n",
            "|    0|[0.39333228673980...|       1.0|\n",
            "|    1|[0.06974989787790...|       1.0|\n",
            "|    0|[0.39417606529206...|       1.0|\n",
            "|    1|[0.03268116134969...|       1.0|\n",
            "|    1|[0.02846366102979...|       1.0|\n",
            "|    1|[0.26410280712989...|       1.0|\n",
            "|    1|[0.01207953987227...|       1.0|\n",
            "|    1|[0.15789917297314...|       1.0|\n",
            "+-----+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## Comprueba que el pipeline anterior funciona correctamente. Para ello realiza una prediccion sobre el conjunto de\n",
        "## datos de trainDF y muestra la prediccion\n",
        "# We transform the data.\n",
        "trainingPred = pModel.transform(trainDF)\n",
        "\n",
        "# # We select the actual label, probability and predictions\n",
        "trainingPred.select('label','probability','prediction').show()"
      ],
      "id": "XYInpJ1kYt62"
    },
    {
      "cell_type": "code",
      "source": [
        "testData = testDF.repartition(10)\n",
        "type(testData)\n",
        "testData.toPandas().to_csv('./heart.csv')"
      ],
      "metadata": {
        "id": "ylyv1AeiaZ_1"
      },
      "id": "ylyv1AeiaZ_1",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Utiliza los csv guardados en data/heart_streaming para simular un proceso de datos en streaming.\n",
        "## Para ello, utiliza la funcion de spark.readStream\n",
        "## En la configuración pon: que se importe un archivo por ejecucion\n",
        "## que se renombre la variable de \"output\"a \"label\"\n",
        "## Llama a este proceso con el nombre sourceStream\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"index\", IntegerType(), True),\n",
        "    StructField(\"age\", IntegerType(), True),\n",
        "    StructField(\"sex\", IntegerType(), True),\n",
        "    StructField(\"cp\", IntegerType(), True),\n",
        "    StructField(\"trestbps\", IntegerType(), True),\n",
        "    StructField(\"chol\", IntegerType(), True),\n",
        "    StructField(\"fbs\", IntegerType(), True),\n",
        "    StructField(\"restecg\", IntegerType(), True),\n",
        "    StructField(\"thalach\", IntegerType(), True),\n",
        "    StructField(\"exang\", IntegerType(), True),\n",
        "    StructField(\"oldpeak\", DoubleType(), True),\n",
        "    StructField(\"slope\", IntegerType(), True),\n",
        "    StructField(\"ca\", IntegerType(), True),\n",
        "    StructField(\"thal\", IntegerType(), True),\n",
        "    StructField(\"target\", IntegerType(), True)  # Renombra la columna \"output\" a \"label\"\n",
        "])\n",
        "\n",
        "sourceStream = (\n",
        "    spark.readStream\n",
        "    .format(\"csv\")\n",
        "    .schema(schema)\n",
        "    .option(\"header\", \"true\")\n",
        "    .option(\"maxFilesPerTrigger\", 1)  # Importa un archivo por ejecución\n",
        "    .load(\"./heart.csv\")\n",
        "    .withColumnRenamed(\"target\", \"label\")  # Renombra la columna \"output\" a \"label\"\n",
        ")"
      ],
      "metadata": {
        "id": "gSvduQBcacfD"
      },
      "id": "gSvduQBcacfD",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Utiliza el pipeline \"pModel\" para realizar predicciones utilizando los datos en streaming de \"sourceStream\"\n",
        "## De la predicción selecciona las variables label, probability, prediction.\n",
        "## Llama a este proceso con el nombre de \"prediction1\"\n",
        "\n",
        "\n",
        "sourceStream.isStreaming"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WtleE3PaevU",
        "outputId": "27869cce-4b55-4f0d-9796-db59e93e82be"
      },
      "id": "_WtleE3PaevU",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction1 = pModel.transform(sourceStream)\n",
        "\n",
        "prediction1.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZfqnJ5yahjF",
        "outputId": "7ead0760-5c8a-41a6-92b0-0337b94e23f3"
      },
      "id": "EZfqnJ5yahjF",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- index: integer (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- sex: integer (nullable = true)\n",
            " |-- cp: integer (nullable = true)\n",
            " |-- trestbps: integer (nullable = true)\n",
            " |-- chol: integer (nullable = true)\n",
            " |-- fbs: integer (nullable = true)\n",
            " |-- restecg: integer (nullable = true)\n",
            " |-- thalach: integer (nullable = true)\n",
            " |-- exang: integer (nullable = true)\n",
            " |-- oldpeak: double (nullable = true)\n",
            " |-- slope: integer (nullable = true)\n",
            " |-- ca: integer (nullable = true)\n",
            " |-- thal: integer (nullable = true)\n",
            " |-- label: integer (nullable = true)\n",
            " |-- features_scaled1: vector (nullable = true)\n",
            " |-- features_scaled: vector (nullable = true)\n",
            " |-- sex_ohe: vector (nullable = true)\n",
            " |-- cp_ohe: vector (nullable = true)\n",
            " |-- fbs_ohe: vector (nullable = true)\n",
            " |-- restecg_ohe: vector (nullable = true)\n",
            " |-- slp_ohe: vector (nullable = true)\n",
            " |-- exng_ohe: vector (nullable = true)\n",
            " |-- caa_ohe: vector (nullable = true)\n",
            " |-- thall_ohe: vector (nullable = true)\n",
            " |-- features: vector (nullable = true)\n",
            " |-- rawPrediction: vector (nullable = true)\n",
            " |-- probability: vector (nullable = true)\n",
            " |-- prediction: double (nullable = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(prediction1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "KiK7KKjuajTo",
        "outputId": "b489b025-cb41-47ed-de6a-341cbd03687a"
      },
      "id": "KiK7KKjuajTo",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[index: int, age: int, sex: int, cp: int, trestbps: int, chol: int, fbs: int, restecg: int, thalach: int, exang: int, oldpeak: double, slope: int, ca: int, thal: int, label: int, features_scaled1: vector, features_scaled: vector, sex_ohe: vector, cp_ohe: vector, fbs_ohe: vector, restecg_ohe: vector, slp_ohe: vector, exng_ohe: vector, caa_ohe: vector, thall_ohe: vector, features: vector, rawPrediction: vector, probability: vector, prediction: double]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mostrando las predicciones en consola"
      ],
      "metadata": {
        "id": "YzuU1PA3arjv"
      },
      "id": "YzuU1PA3arjv"
    },
    {
      "cell_type": "code",
      "source": [
        "## Obten las predicciones sobre los datos en streaming, para ello utiliza prediction1.writeStream. En las opciones de\n",
        "## configuracion pon: \"format\" igual a \"console\"\n",
        "## en .trigger igual (once=True),\n",
        "## y permite que el proceso espere hasta que se complete con .awaitTermination()\n",
        "\n",
        "\n",
        "query = (\n",
        "    prediction1\n",
        "    .writeStream\n",
        "    .outputMode(\"append\")\n",
        "    .format(\"console\")\n",
        "    .trigger(once = True)\n",
        "    .queryName(\"prediction1\")\n",
        "    .start()\n",
        ")\n",
        "\n",
        "query.awaitTermination()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "-kCDuAHDak_R",
        "outputId": "66688de1-ded9-4b07-bedb-e3cdcf8d2d47"
      },
      "id": "-kCDuAHDak_R",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "StreamingQueryException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mStreamingQueryException\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-65b24cdda5bf>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/streaming/query.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mStreamingQueryException\u001b[0m: [STREAM_FAILED] Query [id = 5fc9a390-5eaa-4450-b5f9-6e2f1c2c1811, runId = 76f63751-259f-499a-baf3-d7e991c7654b] terminated with exception: Option 'basePath' must be a directory"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Obten las predicciones sobre los datos en streaming, para ello utiliza prediction1.writeStream.\n",
        "## En las opciones de configuracion pon: que los resultados se guarden en memoria,\n",
        "## que el .outputMode sea \"append\"\n",
        "## que el nombre de la query \"queryName\" sea \"prediction4\"\n",
        "\n",
        "query = (\n",
        "    prediction1\n",
        "    .writeStream\n",
        "    .outputMode(\"append\")\n",
        "    .format(\"memory\")  # You can change the output format as needed\n",
        "    .trigger(once = True)\n",
        "    .queryName(\"prediction4\")\n",
        "    .start()\n",
        ")"
      ],
      "metadata": {
        "id": "cHYPa_4Wa-t0"
      },
      "id": "cHYPa_4Wa-t0",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x in range(2):\n",
        "    df = spark.sql(\n",
        "        \"SELECT * FROM prediction4\")\n",
        "    df.show(10)\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0JOMpOabCFs",
        "outputId": "c3615c38-9c74-48ca-9665-23dab496ec27"
      },
      "id": "E0JOMpOabCFs",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+-----+----------------+---------------+-------+------+-------+-----------+-------+--------+-------+---------+--------+-------------+-----------+----------+\n",
            "|index|age|sex| cp|trestbps|chol|fbs|restecg|thalach|exang|oldpeak|slope| ca|thal|label|features_scaled1|features_scaled|sex_ohe|cp_ohe|fbs_ohe|restecg_ohe|slp_ohe|exng_ohe|caa_ohe|thall_ohe|features|rawPrediction|probability|prediction|\n",
            "+-----+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+-----+----------------+---------------+-------+------+-------+-----------+-------+--------+-------+---------+--------+-------------+-----------+----------+\n",
            "+-----+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+-----+----------------+---------------+-------+------+-------+-----------+-------+--------+-------+---------+--------+-------------+-----------+----------+\n",
            "\n",
            "+-----+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+-----+----------------+---------------+-------+------+-------+-----------+-------+--------+-------+---------+--------+-------------+-----------+----------+\n",
            "|index|age|sex| cp|trestbps|chol|fbs|restecg|thalach|exang|oldpeak|slope| ca|thal|label|features_scaled1|features_scaled|sex_ohe|cp_ohe|fbs_ohe|restecg_ohe|slp_ohe|exng_ohe|caa_ohe|thall_ohe|features|rawPrediction|probability|prediction|\n",
            "+-----+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+-----+----------------+---------------+-------+------+-------+-----------+-------+--------+-------+---------+--------+-------------+-----------+----------+\n",
            "+-----+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+-----+----------------+---------------+-------+------+-------+-----------+-------+--------+-------+---------+--------+-------------+-----------+----------+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[index: int, age: int, sex: int, cp: int, trestbps: int, chol: int, fbs: int, restecg: int, thalach: int, exang: int, oldpeak: double, slope: int, ca: int, thal: int, label: int, features_scaled1: vector, features_scaled: vector, sex_ohe: vector, cp_ohe: vector, fbs_ohe: vector, restecg_ohe: vector, slp_ohe: vector, exng_ohe: vector, caa_ohe: vector, thall_ohe: vector, features: vector, rawPrediction: vector, probability: vector, prediction: double]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Valida que el proceso de streaming está activo y después muestra el estado\n",
        "sourceStream.isStreaming"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J055KkM7bEWe",
        "outputId": "2e18a385-df6b-480a-f1ba-3e7ddc002910"
      },
      "id": "J055KkM7bEWe",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}