{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a86f8df",
   "metadata": {},
   "source": [
    "# Ejercicio Práctico_Predicción en Streaming con Spark ML y Spark Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d06b6c8",
   "metadata": {},
   "source": [
    "En este notebook vamos a cargar un pipeline que tiene un conjunto de fases de pre-procesamiento y un modelo de clasificacion predecir la probabilidad de un paciente de sufrir un ataque al corazón. La predicción se realizará sobre datos en streaming optenidos a partir del csv de heart.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6318a507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f81f759",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56cecab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "## Inicia una sesion de Spark\n",
    "spark = SparkSession.builder.appName('UCI Heart disease').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afe3c31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "|age|sex| cp|trestbps|chol|fbs|restecg|thalach|exang|oldpeak|slope| ca|thal|target|\n",
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "| 63|  1|  3|     145| 233|  1|      0|    150|    0|    2.3|    0|  0|   1|     1|\n",
      "| 37|  1|  2|     130| 250|  0|      1|    187|    0|    3.5|    0|  0|   2|     1|\n",
      "| 41|  0|  1|     130| 204|  0|      0|    172|    0|    1.4|    2|  0|   2|     1|\n",
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Carga y visualiza el csv de Ejercicios\\data\\heart.csv con el nombre de heart\n",
    "heart = spark.read.csv('C:/Users/ilse-/BIG_DATA1/Big_Data1/4. Ejercicios Spark_ML_Koalas/heart.csv', \n",
    "                       inferSchema = True, \n",
    "                       header = True)\n",
    "heart.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8216cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType( \\\n",
    "                     [StructField(\"age\", LongType(),True), \\\n",
    "                      StructField(\"sex\", LongType(), True), \\\n",
    "                      StructField(\"cp\", LongType(), True), \\\n",
    "                      StructField('trestbps', LongType(), True), \\\n",
    "                      StructField(\"chol\", LongType(), True), \\\n",
    "                      StructField(\"fbs\", LongType(), True), \\\n",
    "                      StructField(\"restecg\", LongType(), True), \\\n",
    "                      StructField(\"thalach\", LongType(), True),\\\n",
    "                      StructField(\"exang\", LongType(), True), \\\n",
    "                      StructField(\"oldpeak\", DoubleType(), True), \\\n",
    "                      StructField(\"slope\", LongType(),True), \\\n",
    "                      StructField(\"ca\", LongType(), True), \\\n",
    "                      StructField(\"thal\", LongType(), True), \\\n",
    "                      StructField(\"target\", LongType(), True), \\\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28d8812f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- sex: integer (nullable = true)\n",
      " |-- cp: integer (nullable = true)\n",
      " |-- trestbps: integer (nullable = true)\n",
      " |-- chol: integer (nullable = true)\n",
      " |-- fbs: integer (nullable = true)\n",
      " |-- restecg: integer (nullable = true)\n",
      " |-- thalach: integer (nullable = true)\n",
      " |-- exang: integer (nullable = true)\n",
      " |-- oldpeak: double (nullable = true)\n",
      " |-- slope: integer (nullable = true)\n",
      " |-- ca: integer (nullable = true)\n",
      " |-- thal: integer (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import StructType,StructField,LongType, StringType,DoubleType,TimestampType\n",
    "\n",
    "\n",
    "df = heart.withColumnRenamed(\"target\",\"label\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b2c3b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "testDF, trainDF = df.randomSplit([0.3, 0.7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a8d0d8",
   "metadata": {},
   "source": [
    "### Carga del Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "088afddf",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o43.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/pipelines/metadata\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:204)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\r\n\tat scala.Option.getOrElse(Option.scala:121)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\r\n\tat scala.Option.getOrElse(Option.scala:121)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\r\n\tat org.apache.spark.api.java.JavaRDDLike$class.partitions(JavaRDDLike.scala:61)\r\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-7125e4393d9d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mpModel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPipelineModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\pipelines\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\ilse-\\anaconda3\\envs\\bigdata2\\lib\\site-packages\\pyspark\\ml\\util.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(cls, path)\u001b[0m\n\u001b[0;32m    360\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m         \u001b[1;34m\"\"\"Reads an ML instance from the input path, a shortcut of `read().load(path)`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ilse-\\anaconda3\\envs\\bigdata2\\lib\\site-packages\\pyspark\\ml\\pipeline.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 240\u001b[1;33m         \u001b[0mmetadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDefaultParamsReader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadMetadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    241\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'language'\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'paramMap'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'paramMap'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'language'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'Python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mJavaMLReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ilse-\\anaconda3\\envs\\bigdata2\\lib\\site-packages\\pyspark\\ml\\util.py\u001b[0m in \u001b[0;36mloadMetadata\u001b[1;34m(path, sc, expectedClassName)\u001b[0m\n\u001b[0;32m    556\u001b[0m         \"\"\"\n\u001b[0;32m    557\u001b[0m         \u001b[0mmetadataPath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 558\u001b[1;33m         \u001b[0mmetadataStr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetadataPath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    559\u001b[0m         \u001b[0mloadedVals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDefaultParamsReader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parseMetaData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetadataStr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpectedClassName\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloadedVals\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ilse-\\anaconda3\\envs\\bigdata2\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1376\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1377\u001b[0m         \"\"\"\n\u001b[1;32m-> 1378\u001b[1;33m         \u001b[0mrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1379\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1380\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ilse-\\anaconda3\\envs\\bigdata2\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1325\u001b[0m         \"\"\"\n\u001b[0;32m   1326\u001b[0m         \u001b[0mitems\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m         \u001b[0mtotalParts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m         \u001b[0mpartsScanned\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ilse-\\anaconda3\\envs\\bigdata2\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mgetNumPartitions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    389\u001b[0m         \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m         \"\"\"\n\u001b[1;32m--> 391\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    392\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ilse-\\anaconda3\\envs\\bigdata2\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ilse-\\anaconda3\\envs\\bigdata2\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ilse-\\anaconda3\\envs\\bigdata2\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o43.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/pipelines/metadata\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:204)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\r\n\tat scala.Option.getOrElse(Option.scala:121)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\r\n\tat scala.Option.getOrElse(Option.scala:121)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\r\n\tat org.apache.spark.api.java.JavaRDDLike$class.partitions(JavaRDDLike.scala:61)\r\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "\n",
    "pModel = PipelineModel.load(\"\\pipelines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7be455",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Comprueba que el pipeline anterior funciona correctamente. Para ello realiza una prediccion sobre el conjunto de \n",
    "## datos de trainDF y muestra la prediccion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2077daa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testData = testDF.repartition(10)\n",
    "\n",
    "testData.write.format(\"CSV\").option(\"header\",False).save(\"/heart_streaming/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984bbf3e",
   "metadata": {},
   "source": [
    "## Creando predicciones en Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8489bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utiliza los csv guardados en data/heart_streaming para simular un proceso de datos en streaming.\n",
    "## Para ello, utiliza la funcion de spark.readStream \n",
    "## En la configuración pon: que se importe un archivo por ejecucion\n",
    "## que se renombre la variable de \"output\"a \"label\"\n",
    "## Llama a este proceso con el nombre sourceStream\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c9160b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utiliza el pipeline \"pModel\" para realizar predicciones utilizando los datos en streaming de \"sourceStream\"\n",
    "## De la predicción selecciona las variables label, probability, prediction. \n",
    "## Llama a este proceso con el nombre de \"prediction1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6113aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: bigint, probability: vector, prediction: double]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(prediction1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50112be",
   "metadata": {},
   "source": [
    "#### Mostrando las predicciones en consola"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836cdda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Obten las predicciones sobre los datos en streaming, para ello utiliza prediction1.writeStream. En las opciones de\n",
    "## configuracion pon: \"format\" igual a \"console\" \n",
    "## en .trigger igual (once=True),\n",
    "## y permite que el proceso espere hasta que se complete con .awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d638e3",
   "metadata": {},
   "source": [
    "#### Guardando las perdicciones en Memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2672a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Obten las predicciones sobre los datos en streaming, para ello utiliza prediction1.writeStream. \n",
    "## En las opciones de configuracion pon: que los resultados se guarden en memoria, \n",
    "## que el .outputMode sea \"append\"\n",
    "## que el nombre de la query \"queryName\" sea \"prediction4\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34147a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----------+\n",
      "|label|         probability|prediction|\n",
      "+-----+--------------------+----------+\n",
      "|    1|[0.04086978924170...|       1.0|\n",
      "|    0|[0.98184892212735...|       0.0|\n",
      "|    1|[0.00474279761632...|       1.0|\n",
      "|    1|[0.35775366097494...|       1.0|\n",
      "|    1|[0.05755909903937...|       1.0|\n",
      "|    0|[0.95305536703752...|       0.0|\n",
      "|    0|[0.94079962605713...|       0.0|\n",
      "|    0|[0.13017480179914...|       1.0|\n",
      "|    0|[0.99807916786174...|       0.0|\n",
      "|    1|[0.15541832735450...|       1.0|\n",
      "+-----+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-----+--------------------+----------+\n",
      "|label|         probability|prediction|\n",
      "+-----+--------------------+----------+\n",
      "|    1|[0.04086978924170...|       1.0|\n",
      "|    0|[0.98184892212735...|       0.0|\n",
      "|    1|[0.00474279761632...|       1.0|\n",
      "|    1|[0.35775366097494...|       1.0|\n",
      "|    1|[0.05755909903937...|       1.0|\n",
      "|    0|[0.95305536703752...|       0.0|\n",
      "|    0|[0.94079962605713...|       0.0|\n",
      "|    0|[0.13017480179914...|       1.0|\n",
      "|    0|[0.99807916786174...|       0.0|\n",
      "|    1|[0.15541832735450...|       1.0|\n",
      "+-----+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[label: bigint, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for x in range(2):\n",
    "    df = spark.sql(\n",
    "        \"SELECT * FROM prediction4\")\n",
    "    df.show(10)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9940d9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Valida que el proceso de streaming está activo y después muestra el estado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3391605d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
