{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creación de un RDD desde una lista**\n",
    "\n",
    " \n",
    "\n",
    "En este ejemplo, creamos un RDD a partir de una lista de números y luego aplicamos una transformación para multiplicar cada número por 2. La acción collect() recopila los resultados y los muestra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8, 10]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Crea un contexto Spark\n",
    "sc = SparkContext(\"local\", \"EjemploRDD\")\n",
    "#sc=  SparkContext(appName=\"MyAppName\")\n",
    " \n",
    "# Crea un RDD a partir de una lista\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "\n",
    "# Realiza una transformación: multiplica cada elemento por 2\n",
    "rdd_multiplicado = rdd.map(lambda x: x * 2)\n",
    "\n",
    "# Realiza una acción: muestra los resultados\n",
    "print(rdd_multiplicado.collect())\n",
    "#rdd_multiplicado.collect()\n",
    "# Detén el contexto Spark\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La línea que proporcionaste es un fragmento de código PySpark que inicializa un nuevo SparkContext. Desglosaré cada parte de la línea:\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "**sc = SparkContext(\"local\", \"EjemploRDD\")**\n",
    "\n",
    " \n",
    "\n",
    "* **sc:** Es la variable donde estás guardando tu nuevo SparkContext.\n",
    "\n",
    "* **SparkContext:** Es una clase de PySpark que representa la conexión a un clúster de Spark y se utiliza para crear RDDs, acumuladores y variables de transmisión en ese clúster.\n",
    "\n",
    "* **\"local\":** Es el modo maestro que estás especificando para tu SparkContext. El string \"local\" indica que Spark debería ejecutarse en un modo local de un solo núcleo. Puedes especificar \"local[n]\" para usar n núcleos; por ejemplo, \"local[4]\" usaría 4 núcleos.\n",
    "\n",
    "* **\"EjemploRDD\":** Es el nombre que estás dando a tu aplicación Spark. Este nombre aparecerá en tu interfaz de usuario de Spark y en los registros para ayudarte a diferenciar entre diferentes aplicaciones Spark que podrías estar ejecutando.\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "Resilient Distributed Dataset (RDD) a partir de una lista de Python. Vamos a desglosar cada parte de esta línea:\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "**rdd = sc.parallelize([1, 2, 3, 4, 5])**\n",
    "\n",
    " \n",
    "\n",
    "* **rdd:** Es el nombre que le estás dando a tu nuevo RDD.\n",
    "\n",
    " \n",
    "\n",
    "* **sc:** Es el SparkContext que estás utilizando para crear el RDD. El **SparkContext** es una client-side object que representa la conexión a un clúster de Spark, y se utiliza para crear RDDs, acumuladores y variables de transmisión en ese clúster.\n",
    "\n",
    " \n",
    "\n",
    "* **parallelize:** Es un método de **SparkContext** que crea un nuevo RDD a partir de una colección iterable (en este caso, una lista de Python). El RDD resultante tiene elementos que están distribuidos en los nodos del clúster.\n",
    "\n",
    " \n",
    "\n",
    "* **[1, 2, 3, 4, 5]:** Es la lista de Python que estás usando como fuente de datos para tu RDD. Contiene cinco elementos: los números del 1 al 5\n",
    "\n",
    " \n",
    "\n",
    "**rdd_multiplicado = rdd.map(lambda x: x * 2)**\n",
    "\n",
    " \n",
    "\n",
    "* **rdd_multiplicado:** Es la variable donde estás guardando tu nuevo RDD, que será el resultado de aplicar una función a cada elemento del RDD original (rdd).\n",
    "\n",
    " \n",
    "\n",
    "* **rdd:** Es el RDD original sobre el que estás realizando la operación. Es el RDD que creaste en la línea anterior del código que analizamos anteriormente.\n",
    "\n",
    " \n",
    "\n",
    "* **map:** Es un método que toma una función como argumento y aplica esa función a cada elemento del RDD, produciendo un nuevo RDD.\n",
    "\n",
    " \n",
    "\n",
    "* **lambda x: x * 2:** Es la función que estás aplicando a cada elemento del RDD. Esta función toma un argumento (x) y devuelve el resultado de multiplicar x por 2.\n",
    "\n",
    " \n",
    "\n",
    "**print(rdd_multiplicado.collect())**\n",
    "\n",
    " \n",
    "\n",
    "* **rdd_multiplicado:** Es el RDD sobre el cual estás realizando operaciones. Este RDD fue creado en una línea anterior de tu código y contiene los resultados de multiplicar cada elemento de un RDD original por 2.\n",
    "\n",
    " \n",
    "\n",
    "* **collect():** Es un método que se utiliza para recuperar todos los elementos de un RDD a la memoria del programa driver como una lista de Python. Este es un tipo de acción en Spark, lo que significa que desencadena la ejecución de todas las transformaciones que se han definido anteriormente en el flujo de trabajo de Spark.\n",
    "\n",
    " \n",
    "\n",
    "* **print(...):** Es una función de Python que se utiliza para imprimir su argumento a la consola. En este caso, estás imprimiendo la lista de elementos que fue retornada por rdd_multiplicado.collect()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Carga de un RDD desde un archivo de texto**\n",
    "\n",
    " \n",
    "\n",
    "En este ejemplo, cargamos un archivo de texto como un RDD y luego aplicamos una transformación para filtrar las líneas que contienen la palabra \"Python\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Python, conocido por su simplicidad y versatilidad, es uno de los lenguajes de programación más populares en el mundo de la informática. Su sintaxis legible y su comunidad activa de desarrolladores lo convierten en una elección ideal para una amplia variedad de aplicaciones. ', 'En el campo de la inteligencia artificial, Python se destaca con bibliotecas como TensorFlow y PyTorch, que impulsan avances en el aprendizaje profundo. ', 'Además, Python es una excelente opción para la automatización de tareas, desde la administración de servidores hasta la creación de scripts para simplificar procesos.', 'Con su amplio ecosistema de bibliotecas y su legado en la comunidad de código abierto, Python sigue siendo un lenguaje poderoso y en constante crecimiento que continúa influyendo en numerosos campos de la tecnología y la informática.']\n"
     ]
    }
   ],
   "source": [
    "#from pyspark import SparkContext\n",
    "\n",
    "# Crea un contexto Spark\n",
    "sc = SparkContext(\"local\", \"EjemploRDD\")\n",
    "\n",
    "# Carga un archivo de texto como un RDD\n",
    "rdd = sc.textFile(\"archivo.txt\")\n",
    "\n",
    "\n",
    "# Realiza una transformación: filtra líneas que contienen la palabra \"Python\"\n",
    "rdd_filtrado = rdd.filter(lambda linea: \"Python\" in linea)\n",
    "\n",
    "\n",
    "# Realiza una acción: muestra los resultados\n",
    "print(rdd_filtrado.collect())\n",
    "\n",
    "# Detén el contexto Spark\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reducción de un RDD**\n",
    "\n",
    "En este ejemplo, creamos un RDD a partir de una lista de números y luego aplicamos una reducción para calcular la suma de todos los elementos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "#from pyspark import SparkContext\n",
    "\n",
    "# Crea un contexto Spark\n",
    "sc = SparkContext(\"local\", \"EjemploRDD\")\n",
    "\n",
    "# Crea un RDD a partir de una lista de números\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "\n",
    "# Realiza una reducción para sumar todos los elementos\n",
    "suma = rdd.reduce(lambda x, y: x + y)\n",
    "\n",
    "# Muestra el resultado\n",
    "print(suma)\n",
    "\n",
    "# Detén el contexto Spark\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
