{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark Koalas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este notebook contiene las funciones principales de Koalas, obtenidas de la documentaci√≥n oficial de https://koalas.readthedocs.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\ilse-\\\\anaconda3\\\\envs\\\\bigdata\\\\python.exe'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'bytes' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ilse-\\BIG_DATA1\\Big_Data1\\Ejercicios Apache Spark Koala\\6. Apache Spark Koalas.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ilse-/BIG_DATA1/Big_Data1/Ejercicios%20Apache%20Spark%20Koala/6.%20Apache%20Spark%20Koalas.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mfindspark\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ilse-/BIG_DATA1/Big_Data1/Ejercicios%20Apache%20Spark%20Koala/6.%20Apache%20Spark%20Koalas.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m findspark\u001b[39m.\u001b[39;49minit()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ilse-/BIG_DATA1/Big_Data1/Ejercicios%20Apache%20Spark%20Koala/6.%20Apache%20Spark%20Koalas.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ilse-/BIG_DATA1/Big_Data1/Ejercicios%20Apache%20Spark%20Koala/6.%20Apache%20Spark%20Koalas.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ilse-\\anaconda3\\envs\\bigdata\\lib\\site-packages\\findspark.py:143\u001b[0m, in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Make pyspark importable.\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \n\u001b[0;32m    123\u001b[0m \u001b[39mSets environment variables and adds dependencies to sys.path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[39m    configure and import pyspark.\u001b[39;00m\n\u001b[0;32m    140\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m spark_home:\n\u001b[1;32m--> 143\u001b[0m     spark_home \u001b[39m=\u001b[39m find()\n\u001b[0;32m    145\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m python_path:\n\u001b[0;32m    146\u001b[0m     python_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mPYSPARK_PYTHON\u001b[39m\u001b[39m\"\u001b[39m, sys\u001b[39m.\u001b[39mexecutable)\n",
      "File \u001b[1;32mc:\\Users\\ilse-\\anaconda3\\envs\\bigdata\\lib\\site-packages\\findspark.py:39\u001b[0m, in \u001b[0;36mfind\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m spark_home:\n\u001b[0;32m     37\u001b[0m     \u001b[39m# last resort: try importing pyspark (pip-installed, already on sys.path)\u001b[39;00m\n\u001b[0;32m     38\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 39\u001b[0m         \u001b[39mimport\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[0;32m     41\u001b[0m         \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ilse-\\anaconda3\\envs\\bigdata\\lib\\site-packages\\pyspark\\__init__.py:51\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtypes\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconf\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkConf\n\u001b[1;32m---> 51\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcontext\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkContext\n\u001b[0;32m     52\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrdd\u001b[39;00m \u001b[39mimport\u001b[39;00m RDD, RDDBarrier\n\u001b[0;32m     53\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfiles\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkFiles\n",
      "File \u001b[1;32mc:\\Users\\ilse-\\anaconda3\\envs\\bigdata\\lib\\site-packages\\pyspark\\context.py:31\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtempfile\u001b[39;00m \u001b[39mimport\u001b[39;00m NamedTemporaryFile\n\u001b[0;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpy4j\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprotocol\u001b[39;00m \u001b[39mimport\u001b[39;00m Py4JError\n\u001b[1;32m---> 31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m \u001b[39mimport\u001b[39;00m accumulators\n\u001b[0;32m     32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39maccumulators\u001b[39;00m \u001b[39mimport\u001b[39;00m Accumulator\n\u001b[0;32m     33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbroadcast\u001b[39;00m \u001b[39mimport\u001b[39;00m Broadcast, BroadcastPickleRegistry\n",
      "File \u001b[1;32mc:\\Users\\ilse-\\anaconda3\\envs\\bigdata\\lib\\site-packages\\pyspark\\accumulators.py:97\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39msocketserver\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mSocketServer\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mthreading\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mserializers\u001b[39;00m \u001b[39mimport\u001b[39;00m read_int, PickleSerializer\n\u001b[0;32m    100\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mAccumulator\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mAccumulatorParam\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    103\u001b[0m pickleSer \u001b[39m=\u001b[39m PickleSerializer()\n",
      "File \u001b[1;32mc:\\Users\\ilse-\\anaconda3\\envs\\bigdata\\lib\\site-packages\\pyspark\\serializers.py:71\u001b[0m\n\u001b[0;32m     68\u001b[0m     protocol \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m\n\u001b[0;32m     69\u001b[0m     xrange \u001b[39m=\u001b[39m \u001b[39mrange\u001b[39m\n\u001b[1;32m---> 71\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m \u001b[39mimport\u001b[39;00m cloudpickle\n\u001b[0;32m     72\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m _exception_message\n\u001b[0;32m     75\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mPickleSerializer\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mMarshalSerializer\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mUTF8Deserializer\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\ilse-\\anaconda3\\envs\\bigdata\\lib\\site-packages\\pyspark\\cloudpickle.py:145\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    126\u001b[0m         \u001b[39mreturn\u001b[39;00m types\u001b[39m.\u001b[39mCodeType(\n\u001b[0;32m    127\u001b[0m             co\u001b[39m.\u001b[39mco_argcount,\n\u001b[0;32m    128\u001b[0m             co\u001b[39m.\u001b[39mco_kwonlyargcount,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    141\u001b[0m             (),\n\u001b[0;32m    142\u001b[0m         )\n\u001b[1;32m--> 145\u001b[0m _cell_set_template_code \u001b[39m=\u001b[39m _make_cell_set_template_code()\n\u001b[0;32m    148\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcell_set\u001b[39m(cell, value):\n\u001b[0;32m    149\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Set the value of a closure cell.\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ilse-\\anaconda3\\envs\\bigdata\\lib\\site-packages\\pyspark\\cloudpickle.py:126\u001b[0m, in \u001b[0;36m_make_cell_set_template_code\u001b[1;34m()\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[39mreturn\u001b[39;00m types\u001b[39m.\u001b[39mCodeType(\n\u001b[0;32m    110\u001b[0m         co\u001b[39m.\u001b[39mco_argcount,\n\u001b[0;32m    111\u001b[0m         co\u001b[39m.\u001b[39mco_nlocals,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    123\u001b[0m         (),\n\u001b[0;32m    124\u001b[0m     )\n\u001b[0;32m    125\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 126\u001b[0m     \u001b[39mreturn\u001b[39;00m types\u001b[39m.\u001b[39;49mCodeType(\n\u001b[0;32m    127\u001b[0m         co\u001b[39m.\u001b[39;49mco_argcount,\n\u001b[0;32m    128\u001b[0m         co\u001b[39m.\u001b[39;49mco_kwonlyargcount,\n\u001b[0;32m    129\u001b[0m         co\u001b[39m.\u001b[39;49mco_nlocals,\n\u001b[0;32m    130\u001b[0m         co\u001b[39m.\u001b[39;49mco_stacksize,\n\u001b[0;32m    131\u001b[0m         co\u001b[39m.\u001b[39;49mco_flags,\n\u001b[0;32m    132\u001b[0m         co\u001b[39m.\u001b[39;49mco_code,\n\u001b[0;32m    133\u001b[0m         co\u001b[39m.\u001b[39;49mco_consts,\n\u001b[0;32m    134\u001b[0m         co\u001b[39m.\u001b[39;49mco_names,\n\u001b[0;32m    135\u001b[0m         co\u001b[39m.\u001b[39;49mco_varnames,\n\u001b[0;32m    136\u001b[0m         co\u001b[39m.\u001b[39;49mco_filename,\n\u001b[0;32m    137\u001b[0m         co\u001b[39m.\u001b[39;49mco_name,\n\u001b[0;32m    138\u001b[0m         co\u001b[39m.\u001b[39;49mco_firstlineno,\n\u001b[0;32m    139\u001b[0m         co\u001b[39m.\u001b[39;49mco_lnotab,\n\u001b[0;32m    140\u001b[0m         co\u001b[39m.\u001b[39;49mco_cellvars,  \u001b[39m# this is the trickery\u001b[39;49;00m\n\u001b[0;32m    141\u001b[0m         (),\n\u001b[0;32m    142\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: 'bytes' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import databricks.koalas as ks\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Creaci√≥n de objetos\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creando una serie Koalas pasando una lista de valores, permitiendo que Koalas cree un √≠ndice entero predeterminado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = ks.Series([1, 3, 5, np.nan, 6, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creando un Koalas DataFrame pasando un dict de objetos que se pueden convertir a series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf = ks.DataFrame(\n",
    "    {'a': [1, 2, 3, 4, 5, 6],\n",
    "     'b': [100, 200, 300, 400, 500, 600],\n",
    "     'c': [\"one\", \"two\", \"three\", \"four\", \"five\", \"six\"]},\n",
    "    index=[10, 20, 30, 40, 50, 60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creando un DataFrame de pandas pasando una matriz numpy, con un √≠ndice de fecha y hora y columnas etiquetadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.date_range('20130101', periods=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list('ABCD'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, este DataFrame de pandas se puede convertir en un DataFrame de Koalas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf = ks.from_pandas(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(kdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, se ve y se comporta igual que un DataFrame de pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adem√°s, es posible crear un **Koalas DataFrame desde Spark DataFrame**.\n",
    "\n",
    "Creando un Spark DataFrame a partir de pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creando Koalas DataFrame desde Spark DataFrame.\n",
    "`to_koalas ()` se adjunta autom√°ticamente a Spark DataFrame y est√° disponible como una API cuando se importa Koalas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf = sdf.to_koalas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiene [dtypes] espec√≠ficos. Actualmente se admiten los tipos que son comunes a Spark y pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Manipulaci√≥n de datos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A diferencia de los pandas, los datos en un dataframe de datos de Spark no est√°n _ordenados_, no tienen una noci√≥n intr√≠nseca de √≠ndice. Cuando se le solicite el encabezado, Spark solo tomar√° el n√∫mero solicitado de filas de una partici√≥n. **No hay que utilizar el df de Koalas para devolver filas espec√≠ficas**, use `.loc` o` iloc` en su lugar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muestre el √≠ndice, las columnas y los datos num√©ricos subyacentes.\n",
    "\n",
    "Tambi√©n puede recuperar el √≠ndice; la columna de √≠ndice se puede atribuir a un DataFrame, ver m√°s adelante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Describe** muestra un resumen estad√≠stico r√°pido de sus datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transposici√≥n de sus datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordenando por su √≠ndice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf.sort_index(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordenar por valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf.sort_values(by='B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Datos faltantes\n",
    "Koalas utiliza principalmente el valor `np.nan` para representar los datos faltantes. Por defecto, no se incluye en los c√°lculos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf1 = pdf.reindex(index=dates[0:4], columns=list(pdf.columns) + ['E'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf1.loc[dates[0]:dates[1], 'E'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf1 = ks.from_pandas(pdf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para eliminar las filas que tienen datos faltantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf1.dropna(how='any')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llenando los datos faltantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf1.fillna(value=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Operaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estad√≠sticas\n",
    "Las operaciones en general excluyen los datos faltantes.\n",
    "\n",
    "Realizaci√≥n de una estad√≠stica descriptiva:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuraciones de Spark\n",
    "\n",
    "Varias configuraciones en PySpark se pueden aplicar internamente en Koalas.\n",
    "Por ejemplo, puede habilitar la optimizaci√≥n de Arrow para acelerar enormemente la conversi√≥n de pandas internos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev = spark.conf.get(\"spark.sql.execution.arrow.enabled\")  # Keep its default value.\n",
    "ks.set_option(\"compute.default_index_type\", \"distributed\")  # Use default index prevent overhead.\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # Ignore warnings coming from Arrow optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", True)\n",
    "%timeit ks.range(300000).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", False)\n",
    "%timeit ks.range(300000).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks.reset_option(\"compute.default_index_type\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", prev)  # Set its default value back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agrupaci√≥n\n",
    "Por \"agrupar por\" nos referimos a un proceso que involucra uno o m√°s de los siguientes pasos:\n",
    "\n",
    "- Dividir los datos en grupos seg√∫n algunos criterios.\n",
    "- Aplicar una funci√≥n a cada grupo de forma independiente\n",
    "- Combinar los resultados en una estructura de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf = ks.DataFrame({'A': ['foo', 'bar', 'foo', 'bar',\n",
    "                          'foo', 'bar', 'foo', 'foo'],\n",
    "                    'B': ['one', 'one', 'two', 'three',\n",
    "                          'two', 'two', 'one', 'three'],\n",
    "                    'C': np.random.randn(8),\n",
    "                    'D': np.random.randn(8)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agrupar y luego aplicar el **sum** a los grupos resultantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf.groupby('A').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf.groupby(['A', 'B']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Visualizaci√≥n de datos: Generar gr√°ficos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pser = pd.Series(np.random.randn(1000),\n",
    "                 index=pd.date_range('1/1/2000', periods=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kser = ks.Series(pser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kser = kser.cummax()\n",
    "kser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gr√°ficos de matplotlib**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks.options.plotting.backend = \"matplotlib\"\n",
    "\n",
    "kser.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gr√°ficos de Pandas Bokeh**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks.options.plotting.backend = \"pandas_bokeh\"\n",
    "\n",
    "kdf.plot(backend=\"pandas_bokeh\", title=\"Example Figure\")\n",
    "kser.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gr√°fico por defecto con plotly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kser.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En un DataFrame, el plot() es una conveniencia para trazar todas las columnas con etiquetas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pd.DataFrame(np.random.randn(1000, 4), index=pser.index,\n",
    "                   columns=['A', 'B', 'C', 'D'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf = ks.from_pandas(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf = kdf.cummax()\n",
    "kdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Entrada / salida de datos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV\n",
    "\n",
    "CSV es sencillo y f√°cil de usar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf.to_csv('foo.csv')\n",
    "ks.read_csv('foo.csv').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parquet\n",
    "\n",
    "Parquet es un formato de archivo eficiente y compacto para leer y escribir m√°s r√°pido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf.to_parquet('bar.parquet')\n",
    "ks.read_parquet('bar.parquet').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark IO\n",
    "\n",
    "Adem√°s, Koalas es totalmente compatible con las diversas fuentes de datos de Spark, como ORC y una fuente de datos externa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf.to_spark_io('zoo.orc', format=\"orc\")\n",
    "ks.read_spark_io('zoo.orc', format=\"orc\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
